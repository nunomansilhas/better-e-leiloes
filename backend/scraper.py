"""
Web Scraper para E-Leiloes.pt usando Playwright
"""

import sys
import asyncio

# Fix para Windows - asyncio subprocess com Playwright
# CRÃTICO: WindowsProactorEventLoopPolicy suporta subprocessos
if sys.platform == 'win32':
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

from typing import List, Optional
from datetime import datetime
import re
from playwright.async_api import async_playwright, Page, Browser
import os

from models import EventData, GPSCoordinates, EventDetails, ValoresLeilao, ScraperStatus


class EventScraper:
    """Scraper assÃ­ncrono para e-leiloes.pt"""
    
    def __init__(self):
        self.browser: Optional[Browser] = None
        self.playwright = None
        
        # Status tracking
        self.is_running = False
        self.stop_requested = False
        self.events_processed = 0
        self.events_failed = 0
        self.current_page = None
        self.started_at = None
        self.last_update = None
        
        # Config
        self.delay = float(os.getenv("SCRAPE_DELAY", 0.8))
        self.concurrent = int(os.getenv("CONCURRENT_REQUESTS", 4))
        self.user_agent = os.getenv("USER_AGENT", "Mozilla/5.0 (Windows NT 10.0; Win64; x64)")
    
    async def init_browser(self):
        """Inicializa browser Playwright"""
        if not self.browser:
            self.playwright = await async_playwright().start()
            self.browser = await self.playwright.chromium.launch(
                headless=True,
                args=['--disable-blink-features=AutomationControlled']
            )
    
    async def close(self):
        """Fecha browser"""
        if self.browser:
            await self.browser.close()
        if self.playwright:
            await self.playwright.stop()

    def stop(self):
        """Solicita parada do scraping"""
        print("ðŸ›‘ Paragem do scraper solicitada...")
        self.stop_requested = True

    async def scrape_event(self, reference: str) -> EventData:
        """
        Scrape pÃºblico de um Ãºnico evento por referÃªncia.
        Detecta automaticamente se Ã© imÃ³vel (LO) ou mÃ³vel (NP) pela referÃªncia.

        Args:
            reference: ReferÃªncia do evento (ex: LO1234567890 ou NP1234567890)

        Returns:
            EventData completo do evento
        """
        await self.init_browser()

        # Determina tipo baseado no prefixo da referÃªncia
        # LO = LeilÃ£o Online (geralmente imÃ³veis)
        # NP = NegociaÃ§Ã£o Particular (pode ser mÃ³veis ou imÃ³veis)
        # Para seguranÃ§a, vamos tentar buscar a pÃ¡gina e detectar o tipo
        tipo_evento = "imovel" if reference.startswith("LO") else "imovel"  # default imovel

        # Cria preview fake (valores virÃ£o da pÃ¡gina individual)
        preview = {
            'reference': reference,
            'valores': ValoresLeilao()  # Vazio, serÃ¡ preenchido na pÃ¡gina
        }

        try:
            return await self._scrape_event_details(preview, tipo_evento)
        except Exception as e:
            raise Exception(f"Erro ao fazer scrape do evento {reference}: {str(e)}")

    async def _scrape_event_details(self, preview: dict, tipo_evento: str) -> EventData:
        """
        FASE 2: Entra na pÃ¡gina individual para extrair detalhes completos
        
        Args:
            preview: Dict com {reference, valores} da listagem
            tipo_evento: "imovel" ou "movel"
        """
        reference = preview['reference']
        valores_listagem = preview['valores']
        
        url = f"https://www.e-leiloes.pt/evento/{reference}"
        
        context = await self.browser.new_context(
            user_agent=self.user_agent,
            viewport={'width': 1920, 'height': 1080}
        )
        
        page = await context.new_page()
        
        try:
            # Navega para pÃ¡gina do evento
            await page.goto(url, wait_until="networkidle", timeout=15000)
            await asyncio.sleep(1.5)

            # Extrai datas do evento
            data_inicio, data_fim = await self._extract_dates(page)

            # GPS (apenas para imÃ³veis)
            gps = None
            if tipo_evento == "imovel":
                gps = await self._extract_gps(page)

            # Detalhes (diferente para imovel vs movel)
            if tipo_evento == "imovel":
                detalhes = await self._extract_imovel_details(page)
            else:
                detalhes = await self._extract_movel_details(page)

            # Confirma/atualiza valores na pÃ¡gina individual (podem ser mais precisos)
            valores_pagina = await self._extract_valores_from_page(page)

            # Merge valores (prioridade: pÃ¡gina individual > listagem)
            valores_final = ValoresLeilao(
                valorBase=valores_pagina.valorBase or valores_listagem.valorBase,
                valorAbertura=valores_pagina.valorAbertura or valores_listagem.valorAbertura,
                valorMinimo=valores_pagina.valorMinimo or valores_listagem.valorMinimo,
                lanceAtual=valores_pagina.lanceAtual or valores_listagem.lanceAtual
            )

            # Extrai novas informaÃ§Ãµes
            imagens = await self._extract_gallery(page)
            descricao = await self._extract_descricao(page)
            observacoes = await self._extract_observacoes(page)
            descricao_predial = await self._extract_descricao_predial(page)
            cerimonia = await self._extract_cerimonia(page)
            agente = await self._extract_agente(page)
            dados_processo = await self._extract_dados_processo(page)

            return EventData(
                reference=reference,
                tipoEvento=tipo_evento,
                valores=valores_final,
                gps=gps,
                detalhes=detalhes,
                dataInicio=data_inicio,
                dataFim=data_fim,
                imagens=imagens,
                descricao=descricao,
                observacoes=observacoes,
                descricaoPredial=descricao_predial,
                cerimoniaEncerramento=cerimonia,
                agenteExecucao=agente,
                dadosProcesso=dados_processo,
                scraped_at=datetime.utcnow()
            )
            
        except Exception as e:
            raise Exception(f"Erro ao scrape {reference}: {str(e)}")
        
        finally:
            await page.close()
            await context.close()
    
    async def _extract_dates(self, page: Page) -> tuple[Optional[datetime], Optional[datetime]]:
        """Extrai datas de inÃ­cio e fim do evento do DOM da pÃ¡gina"""
        try:
            data_inicio = None
            data_fim = None

            # Procura por divs que contenham as datas
            # Estrutura: <div><span>InÃ­cio:</span><span class="font-semibold">DATA</span></div>
            divs = await page.query_selector_all('div.flex.justify-content-between')

            for div in divs:
                text = await div.text_content()
                if not text:
                    continue

                text = text.strip()

                # Verifica se Ã© a div de InÃ­cio
                if 'InÃ­cio:' in text:
                    # Busca o span com font-semibold dentro desta div
                    date_span = await div.query_selector('span.font-semibold')
                    if date_span:
                        value = await date_span.text_content()
                        if value:
                            try:
                                # Parse data no formato DD/MM/YYYY HH:MM:SS
                                data_inicio = datetime.strptime(value.strip(), '%d/%m/%Y %H:%M:%S')
                            except ValueError as e:
                                print(f"âš ï¸ Erro ao parsear data de inÃ­cio '{value}': {e}")

                # Verifica se Ã© a div de Fim
                elif 'Fim:' in text:
                    # Busca o span com font-semibold dentro desta div
                    date_span = await div.query_selector('span.font-semibold')
                    if date_span:
                        value = await date_span.text_content()
                        if value:
                            try:
                                # Parse data no formato DD/MM/YYYY HH:MM:SS
                                data_fim = datetime.strptime(value.strip(), '%d/%m/%Y %H:%M:%S')
                            except ValueError as e:
                                print(f"âš ï¸ Erro ao parsear data de fim '{value}': {e}")

            return data_inicio, data_fim

        except Exception as e:
            print(f"âš ï¸ Erro ao extrair datas: {e}")
            return None, None

    async def _extract_gps(self, page: Page) -> GPSCoordinates:
        """Extrai coordenadas GPS do DOM da pÃ¡gina (apenas da seÃ§Ã£o LocalizaÃ§Ã£o)"""
        try:
            latitude = None
            longitude = None

            # Primeiro, encontra a div com tÃ­tulo "LocalizaÃ§Ã£o"
            localizacao_section = None
            title_divs = await page.query_selector_all('.font-semibold.text-xl')

            for title_div in title_divs:
                text = await title_div.text_content()
                if text and 'LocalizaÃ§Ã£o' in text.strip():
                    # Pega o elemento pai (a seÃ§Ã£o completa de localizaÃ§Ã£o)
                    localizacao_section = await title_div.evaluate_handle(
                        'el => el.closest(".flex.flex-column.w-full")'
                    )
                    break

            if not localizacao_section:
                print("âš ï¸ SeÃ§Ã£o 'LocalizaÃ§Ã£o' nÃ£o encontrada")
                return GPSCoordinates(latitude=None, longitude=None)

            # Agora procura GPS apenas dentro desta seÃ§Ã£o
            spans = await localizacao_section.query_selector_all('.font-semibold')

            for span in spans:
                text = await span.text_content()
                if not text:
                    continue

                text = text.strip()

                if text == 'GPS Latitude:':
                    # Pega prÃ³ximo elemento (o valor)
                    next_el = await span.evaluate_handle('el => el.nextElementSibling')
                    if next_el:
                        value = await next_el.text_content()
                        if value:
                            try:
                                latitude = float(value.strip())
                            except ValueError:
                                pass

                elif text == 'GPS Longitude:':
                    next_el = await span.evaluate_handle('el => el.nextElementSibling')
                    if next_el:
                        value = await next_el.text_content()
                        if value:
                            try:
                                longitude = float(value.strip())
                            except ValueError:
                                pass

            return GPSCoordinates(latitude=latitude, longitude=longitude)

        except Exception as e:
            print(f"âš ï¸ Erro ao extrair GPS: {e}")
            return GPSCoordinates(latitude=None, longitude=None)

    async def _extract_gallery(self, page: Page) -> List[str]:
        """Extrai todas as URLs das imagens da galeria"""
        try:
            images = []
            import re

            # Aguarda a galeria carregar
            try:
                await page.wait_for_selector('.p-galleria', timeout=3000)
            except:
                print("âš ï¸ Galeria nÃ£o encontrada")

            # MÃ©todo 1: Procura por items da galeria com IDs (pv_id_*_item_*)
            # Extrai o ID base do primeiro item para descobrir quantos items existem
            first_item = await page.query_selector('.p-galleria-item[id]')
            if first_item:
                first_id = await first_item.get_attribute('id')
                if first_id:
                    # Extrai o prefixo do ID (ex: "pv_id_360" de "pv_id_360_item_0")
                    id_match = re.match(r'(pv_id_\d+)_item_\d+', first_id)
                    if id_match:
                        id_prefix = id_match.group(1)
                        print(f"ðŸ” Procurando imagens com prefixo: {id_prefix}")

                        # Tenta encontrar todos os items (atÃ© 50 imagens)
                        consecutive_misses = 0
                        for i in range(50):
                            item_id = f"{id_prefix}_item_{i}"
                            item = await page.query_selector(f'#{item_id} .p-evento-image')
                            if item:
                                consecutive_misses = 0
                                style = await item.get_attribute('style')
                                if style and 'background-image: url(' in style:
                                    match = re.search(r'url\(["\']?([^"\']+)["\']?\)', style)
                                    if match:
                                        url = match.group(1).replace('&quot;', '')
                                        if url and url not in images:
                                            images.append(url)
                            else:
                                consecutive_misses += 1
                                # Parar apÃ³s 3 tentativas consecutivas sem sucesso
                                if consecutive_misses >= 3:
                                    break

            # MÃ©todo 2: Fallback - procura por todas as imagens visÃ­veis
            if len(images) == 0:
                print("ðŸ”„ Usando mÃ©todo fallback para imagens")
                gallery_items = await page.query_selector_all('.p-galleria-item .p-evento-image, .p-evento-image')

                for item in gallery_items:
                    style = await item.get_attribute('style')
                    if style and 'background-image: url(' in style:
                        match = re.search(r'url\(["\']?([^"\']+)["\']?\)', style)
                        if match:
                            url = match.group(1).replace('&quot;', '')
                            if url and url not in images:
                                images.append(url)

            # MÃ©todo 3: Tenta buscar via API de imagens no HTML
            if len(images) == 0:
                try:
                    print("ðŸ”„ Tentando extrair via API de fotos")
                    page_content = await page.content()
                    api_matches = re.findall(r'/api/files/Verbas_Fotos/[^"\'<>\s]+', page_content)
                    for url in api_matches:
                        if url not in images:
                            # Garante URL completo
                            if not url.startswith('http'):
                                url = f"https://www.e-leiloes.pt{url}"
                            images.append(url)
                except Exception as e:
                    print(f"âš ï¸ Erro ao buscar API de fotos: {e}")

            print(f"ðŸ“· Galeria: {len(images)} imagens encontradas")
            return images

        except Exception as e:
            print(f"âš ï¸ Erro ao extrair galeria: {e}")
            return []

    async def _extract_descricao(self, page: Page) -> Optional[str]:
        """Extrai a descriÃ§Ã£o completa do bem"""
        try:
            # Procura pela seÃ§Ã£o "DescriÃ§Ã£o"
            title_divs = await page.query_selector_all('.font-semibold.text-xl')

            for title_div in title_divs:
                text = await title_div.text_content()
                if text and 'DescriÃ§Ã£o' in text.strip():
                    # Pega o elemento pai (a seÃ§Ã£o completa)
                    section = await title_div.evaluate_handle(
                        'el => el.closest(".flex.flex-column.w-full")'
                    )
                    if section:
                        # Pega o prÃ³ximo div apÃ³s o tÃ­tulo (que contÃ©m a descriÃ§Ã£o)
                        desc_div = await section.query_selector('div:not(.flex.flex-column)')
                        if desc_div:
                            descricao = await desc_div.text_content()
                            return descricao.strip() if descricao else None

            return None

        except Exception as e:
            print(f"âš ï¸ Erro ao extrair descriÃ§Ã£o: {e}")
            return None

    async def _extract_observacoes(self, page: Page) -> Optional[str]:
        """Extrai as observaÃ§Ãµes sobre o evento"""
        try:
            # Procura pela seÃ§Ã£o "ObservaÃ§Ãµes"
            title_divs = await page.query_selector_all('.font-semibold.text-xl')

            for title_div in title_divs:
                text = await title_div.text_content()
                if text and 'ObservaÃ§Ãµes' in text.strip() or 'Observacoes' in text.strip():
                    # Pega o elemento pai (a seÃ§Ã£o completa)
                    section = await title_div.evaluate_handle(
                        'el => el.closest(".flex.flex-column.w-full")'
                    )
                    if section:
                        # Pega o prÃ³ximo div apÃ³s o tÃ­tulo (que contÃ©m as observaÃ§Ãµes)
                        obs_div = await section.query_selector('div:not(.flex.flex-column)')
                        if obs_div:
                            observacoes = await obs_div.text_content()
                            return observacoes.strip() if observacoes else None

            return None

        except Exception as e:
            print(f"âš ï¸ Erro ao extrair observaÃ§Ãµes: {e}")
            return None

    async def _extract_descricao_predial(self, page: Page):
        """Extrai informaÃ§Ã£o da descriÃ§Ã£o predial"""
        try:
            from models import DescricaoPredial

            # Procura pela seÃ§Ã£o "DescriÃ§Ã£o Predial"
            title_divs = await page.query_selector_all('.font-semibold.text-xl')

            for title_div in title_divs:
                text = await title_div.text_content()
                if text and 'DescriÃ§Ã£o Predial' in text.strip():
                    section = await title_div.evaluate_handle(
                        'el => el.closest(".flex.flex-column.w-full")'
                    )
                    if not section:
                        continue

                    # Extrai campos
                    numero_desc = None
                    fracao = None
                    distrito_code = None
                    concelho_code = None
                    freguesia_code = None
                    artigos = []

                    spans = await section.query_selector_all('.font-semibold')
                    for span in spans:
                        label = await span.text_content()
                        if not label:
                            continue

                        label = label.strip()

                        # Pega o prÃ³ximo elemento (o valor)
                        next_el = await span.evaluate_handle('el => el.nextElementSibling')
                        if next_el:
                            value = await next_el.text_content()
                            if value:
                                value = value.strip()

                                if 'N.Âº da DescriÃ§Ã£o:' in label:
                                    numero_desc = value
                                elif 'FraÃ§Ã£o:' in label:
                                    fracao = value if value else None
                                elif 'Distrito:' in label:
                                    distrito_code = value
                                elif 'Concelho:' in label:
                                    concelho_code = value
                                elif 'Freguesia:' in label:
                                    freguesia_code = value

                    return DescricaoPredial(
                        numeroDescricao=numero_desc,
                        fracao=fracao,
                        distritoCode=distrito_code,
                        concelhoCode=concelho_code,
                        freguesiaCode=freguesia_code,
                        artigos=artigos
                    )

            return None

        except Exception as e:
            print(f"âš ï¸ Erro ao extrair descriÃ§Ã£o predial: {e}")
            return None

    async def _extract_cerimonia(self, page: Page):
        """Extrai dados da cerimÃ³nia de encerramento"""
        try:
            from models import CerimoniaEncerramento

            # Procura pela seÃ§Ã£o "Cerimonia de Encerramento"
            title_divs = await page.query_selector_all('.font-semibold.text-xl')

            for title_div in title_divs:
                text = await title_div.text_content()
                if text and 'Cerimonia' in text.strip() or 'CerimÃ³nia' in text.strip():
                    section = await title_div.evaluate_handle(
                        'el => el.closest(".flex.flex-column.w-full")'
                    )
                    if not section:
                        continue

                    data = None
                    local = None
                    morada = None

                    spans = await section.query_selector_all('.font-semibold')
                    for span in spans:
                        label = await span.text_content()
                        if not label:
                            continue

                        label = label.strip()

                        next_el = await span.evaluate_handle('el => el.nextElementSibling')
                        if next_el:
                            value = await next_el.text_content()
                            if value:
                                value = value.strip()

                                if 'Data:' in label:
                                    try:
                                        # Parse ISO datetime
                                        data = datetime.fromisoformat(value.replace('Z', '+00:00'))
                                    except:
                                        pass
                                elif 'Local:' in label:
                                    local = value
                                elif 'Morada:' in label:
                                    morada = value

                    return CerimoniaEncerramento(
                        data=data,
                        local=local,
                        morada=morada
                    )

            return None

        except Exception as e:
            print(f"âš ï¸ Erro ao extrair cerimÃ³nia: {e}")
            return None

    async def _extract_agente(self, page: Page):
        """Extrai dados do agente de execuÃ§Ã£o"""
        try:
            from models import AgenteExecucao

            # Procura pela seÃ§Ã£o "Agente de ExecuÃ§Ã£o"
            title_divs = await page.query_selector_all('.font-semibold.text-xl')

            for title_div in title_divs:
                text = await title_div.text_content()
                if text and 'Agente de ExecuÃ§Ã£o' in text.strip() or 'Agente de Execucao' in text.strip():
                    section = await title_div.evaluate_handle(
                        'el => el.closest(".flex.flex-column.w-full")'
                    )
                    if not section:
                        continue

                    # Pega todos os spans dentro da seÃ§Ã£o
                    all_spans = await section.query_selector_all('span')
                    nome = None
                    email = None
                    telefone = None

                    for span in all_spans:
                        value = await span.text_content()
                        if not value:
                            continue

                        value = value.strip()

                        # Detecta email
                        if '@' in value:
                            email = value
                        # Detecta telefone (simples)
                        elif value.replace(' ', '').isdigit() and len(value.replace(' ', '')) >= 9:
                            telefone = value
                        # Primeiro span nÃ£o vazio Ã© o nome
                        elif not nome and len(value) > 3:
                            nome = value

                    return AgenteExecucao(
                        nome=nome,
                        email=email,
                        telefone=telefone
                    )

            return None

        except Exception as e:
            print(f"âš ï¸ Erro ao extrair agente: {e}")
            return None

    async def _extract_dados_processo(self, page: Page):
        """Extrai dados do processo judicial"""
        try:
            from models import DadosProcesso

            # Procura pela seÃ§Ã£o "Dados do Processo"
            title_divs = await page.query_selector_all('.font-semibold.text-xl')

            for title_div in title_divs:
                text = await title_div.text_content()
                if text and 'Dados do Processo' in text.strip():
                    section = await title_div.evaluate_handle(
                        'el => el.closest(".flex.flex-column.w-full")'
                    )
                    if not section:
                        continue

                    processo = None
                    tribunal = None
                    unidade_organica = None
                    requerentes = []

                    spans = await section.query_selector_all('.font-semibold')
                    for span in spans:
                        label = await span.text_content()
                        if not label:
                            continue

                        label = label.strip()

                        next_el = await span.evaluate_handle('el => el.nextElementSibling')
                        if next_el:
                            value = await next_el.text_content()
                            if value:
                                value = value.strip()

                                if 'Processo:' in label:
                                    processo = value
                                elif 'Tribunal:' in label:
                                    tribunal = value
                                elif 'Unidade OrgÃ¢nica:' in label or 'Unidade Organica:' in label:
                                    unidade_organica = value
                                elif 'Requerentes' in label or 'Requeridos' in label or 'Interessados' in label:
                                    # Pega todos os prÃ³ximos spans com nomes
                                    parent = await next_el.evaluate_handle('el => el.parentElement')
                                    if parent:
                                        name_spans = await parent.query_selector_all('span:not(.font-semibold)')
                                        for name_span in name_spans:
                                            name = await name_span.text_content()
                                            if name and name.strip() and len(name.strip()) > 3:
                                                requerentes.append(name.strip())

                    return DadosProcesso(
                        processo=processo,
                        tribunal=tribunal,
                        unidadeOrganica=unidade_organica,
                        requerentes=requerentes
                    )

            return None

        except Exception as e:
            print(f"âš ï¸ Erro ao extrair dados do processo: {e}")
            return None

    async def _extract_valores_from_page(self, page: Page) -> ValoresLeilao:
        """Extrai valores da pÃ¡gina individual do evento"""
        valores = ValoresLeilao()
        
        try:
            # Procura por elementos com valores
            body_text = await page.text_content('body')
            
            # Regex para valores monetÃ¡rios
            value_patterns = {
                'valorBase': r'(?:valor\s+base|base)[:\s]*â‚¬?\s*([\d\s.]+,\d{2})',
                'valorAbertura': r'(?:valor\s+abertura|abertura)[:\s]*â‚¬?\s*([\d\s.]+,\d{2})',
                'valorMinimo': r'(?:valor\s+m[iÃ­]nimo|m[iÃ­]nimo)[:\s]*â‚¬?\s*([\d\s.]+,\d{2})',
                'lanceAtual': r'(?:lance\s+atual|atual)[:\s]*â‚¬?\s*([\d\s.]+,\d{2})'
            }
            
            for field, pattern in value_patterns.items():
                match = re.search(pattern, body_text, re.IGNORECASE)
                if match:
                    value_str = match.group(1).replace(' ', '').replace('.', '').replace(',', '.')
                    setattr(valores, field, float(value_str))
                    
        except Exception as e:
            print(f"âš ï¸ Erro ao extrair valores da pÃ¡gina: {e}")
        
        return valores
    
    async def _extract_imovel_details(self, page: Page) -> EventDetails:
        """Extrai detalhes COMPLETOS do IMÃ“VEL via DOM"""
        
        async def extract_detail(label: str) -> str:
            """Extrai valor de um campo especÃ­fico"""
            try:
                # Procura por span.font-semibold com o label
                spans = await page.query_selector_all('.flex.w-full .font-semibold')
                
                for span in spans:
                    text = await span.text_content()
                    if text and text.strip() == label:
                        # Pega prÃ³ximo elemento
                        next_el = await span.evaluate_handle('el => el.nextElementSibling')
                        if next_el:
                            value = await next_el.text_content()
                            return value.strip() if value else "N/A"
                
                return "N/A"
            except:
                return "N/A"
        
        async def extract_area(label: str) -> Optional[float]:
            """Extrai Ã¡rea em mÂ²"""
            try:
                spans = await page.query_selector_all('.flex.w-full .font-semibold')
                
                for span in spans:
                    text = await span.text_content()
                    if text and text.strip() == label:
                        # Procura por span.mr-1 no wrapper
                        wrapper = await span.evaluate_handle('el => el.closest(".flex.w-full")')
                        if wrapper:
                            number_span = await wrapper.query_selector('span.mr-1')
                            if number_span:
                                value = await number_span.text_content()
                                # Remove espaÃ§os e converte para float
                                return float(value.replace(',', '.').strip())
                
                return None
            except:
                return None
        
        # Extrai todos os campos
        tipo = await extract_detail('Tipo:')
        subtipo = await extract_detail('Subtipo:')
        tipologia = await extract_detail('Tipologia:')
        distrito = await extract_detail('Distrito:')
        concelho = await extract_detail('Concelho:')
        freguesia = await extract_detail('Freguesia:')
        
        area_priv = await extract_area('Ãrea Privativa:')
        area_dep = await extract_area('Ãrea Dependente:')
        area_total = await extract_area('Ãrea Total:')
        
        return EventDetails(
            tipo=tipo,
            subtipo=subtipo,
            tipologia=tipologia,
            areaPrivativa=area_priv,
            areaDependente=area_dep,
            areaTotal=area_total,
            distrito=distrito,
            concelho=concelho,
            freguesia=freguesia
        )
    
    async def _extract_movel_details(self, page: Page) -> EventDetails:
        """Extrai detalhes SIMPLIFICADOS do MÃ“VEL (automÃ³vel) via DOM"""
        
        async def extract_detail(label: str) -> str:
            """Extrai valor de um campo especÃ­fico"""
            try:
                spans = await page.query_selector_all('.flex.w-full .font-semibold')
                
                for span in spans:
                    text = await span.text_content()
                    if text and text.strip() == label:
                        next_el = await span.evaluate_handle('el => el.nextElementSibling')
                        if next_el:
                            value = await next_el.text_content()
                            return value.strip() if value else "N/A"
                
                return "N/A"
            except:
                return "N/A"
        
        # Extrai apenas os 3 campos necessÃ¡rios para mÃ³veis
        tipo = await extract_detail('Tipo:')
        subtipo = await extract_detail('Subtipo:')
        matricula = await extract_detail('MatrÃ­cula:')
        
        return EventDetails(
            tipo=tipo,
            subtipo=subtipo,
            matricula=matricula
        )
    
    async def scrape_all_events(self, max_pages: Optional[int] = None) -> List[EventData]:
        """
        Scrape TODOS os eventos (IMOVEIS + MOVEIS) do site

        Args:
            max_pages: MÃ¡ximo de pÃ¡ginas para processar POR TIPO (None = todas)

        Returns:
            Lista com todos os eventos
        """
        self.is_running = True
        self.stop_requested = False
        self.started_at = datetime.utcnow()
        self.events_processed = 0
        self.events_failed = 0

        all_events = []

        await self.init_browser()

        try:
            # 1. SCRAPE IMOVEIS (tipo=1)
            if not self.stop_requested:
                print("ðŸ  Iniciando scraping de IMÃ“VEIS...")
                imoveis = await self._scrape_by_type(tipo=1, max_pages=max_pages)
                all_events.extend(imoveis)
                print(f"âœ… ImÃ³veis recolhidos: {len(imoveis)}")

            # 2. SCRAPE MOVEIS (tipo=2)
            if not self.stop_requested:
                print("ðŸš— Iniciando scraping de MÃ“VEIS...")
                moveis = await self._scrape_by_type(tipo=2, max_pages=max_pages)
                all_events.extend(moveis)
                print(f"âœ… MÃ³veis recolhidos: {len(moveis)}")

            if self.stop_requested:
                print(f"âš ï¸ Scraping interrompido pelo utilizador. Total processado: {len(all_events)}")
            else:
                print(f"ðŸŽ‰ Total de eventos: {len(all_events)}")

            return all_events

        finally:
            self.is_running = False
            self.stop_requested = False
    
    async def _scrape_by_type(self, tipo: int, max_pages: Optional[int] = None) -> List[EventData]:
        """
        Scrape eventos de um tipo especÃ­fico (1=imovel, 2=movel)
        
        FASE 1: Extrai referÃªncias + valores da listagem
        FASE 2: Entra em cada evento para detalhes + GPS
        """
        tipo_nome = "imovel" if tipo == 1 else "movel"
        
        # FASE 1: PÃ¡gina de listagem
        events_preview = await self._extract_from_listing(tipo, max_pages)
        print(f"ðŸ“‹ {len(events_preview)} eventos {tipo_nome} encontrados na listagem")
        
        # FASE 2: PÃ¡gina individual (paralelo)
        all_events = []

        for i in range(0, len(events_preview), self.concurrent):
            # Verifica se foi solicitada paragem
            if self.stop_requested:
                print(f"ðŸ›‘ Scraping interrompido na pÃ¡gina {self.current_page}")
                break

            batch = events_preview[i:i + self.concurrent]

            tasks = [
                self._scrape_event_details(preview, tipo_nome)
                for preview in batch
            ]
            results = await asyncio.gather(*tasks, return_exceptions=True)

            for result in results:
                if isinstance(result, EventData):
                    all_events.append(result)
                    self.events_processed += 1
                    self.last_update = datetime.utcnow()
                else:
                    self.events_failed += 1
                    print(f"âš ï¸ Erro: {result}")

            print(f"ðŸ“Š Processados: {self.events_processed} eventos {tipo_nome}")
            await asyncio.sleep(self.delay)

        return all_events
    
    async def _extract_from_listing(self, tipo: int, max_pages: Optional[int]) -> List[dict]:
        """
        FASE 1: Extrai referÃªncias + valores da pÃ¡gina de listagem
        
        Usa paginaÃ§Ã£o com first=0, first=12, first=24, etc. (12 eventos por pÃ¡gina)
        
        Returns:
            Lista de dicts com {reference, valores}
        """
        context = await self.browser.new_context(user_agent=self.user_agent)
        page = await context.new_page()
        
        events_preview = []
        
        try:
            page_num = 0  # ComeÃ§a em 0
            first_offset = 0  # Offset inicial
            
            while True:
                # Verifica se foi solicitada paragem
                if self.stop_requested:
                    print(f"ðŸ›‘ Scraping da listagem interrompido")
                    break

                self.current_page = page_num + 1  # Para display (pÃ¡gina 1, 2, 3...)

                # Navega para pÃ¡gina de listagem com offset correto
                # https://www.e-leiloes.pt/eventos?layout=grid&first=0&sort=dataFimAsc&tipo=1
                url = f"https://www.e-leiloes.pt/eventos?layout=grid&first={first_offset}&sort=dataFimAsc&tipo={tipo}"
                print(f"ðŸŒ Navegando para pÃ¡gina {page_num + 1} (first={first_offset})...")
                await page.goto(url, wait_until="networkidle")
                await asyncio.sleep(1.5)
                
                # Extrai cards
                cards = await page.query_selector_all('.p-evento')
                
                if not cards:
                    print(f"ðŸ“„ PÃ¡gina {page_num} vazia - fim")
                    break
                
                count_before = len(events_preview)
                
                for card in cards:
                    try:
                        # ReferÃªncia
                        ref_el = await card.query_selector('.pi-tag + span')
                        if not ref_el:
                            continue
                        
                        reference = (await ref_el.text_content()).strip()
                        
                        # Verifica duplicado
                        if any(e['reference'] == reference for e in events_preview):
                            continue
                        
                        # Extrai valores
                        valores = await self._extract_valores_from_card(card)
                        
                        events_preview.append({
                            'reference': reference,
                            'valores': valores
                        })
                        
                    except Exception as e:
                        print(f"âš ï¸ Erro ao extrair card: {e}")
                        continue
                
                count_new = len(events_preview) - count_before
                print(f"ðŸ“„ PÃ¡gina {page_num + 1}: +{count_new} eventos (total: {len(events_preview)})")
                
                if count_new == 0:
                    break
                
                if max_pages and (page_num + 1) >= max_pages:
                    print(f"ðŸ“„ Limite de {max_pages} pÃ¡ginas atingido")
                    break
                
                # Incrementa offset de 12 em 12
                page_num += 1
                first_offset += 12
            
            return events_preview
            
        finally:
            await page.close()
            await context.close()
    
    async def _extract_valores_from_card(self, card) -> ValoresLeilao:
        """Extrai valores (Base, Abertura, MÃ­nimo, Lance Atual) de um card"""
        valores = ValoresLeilao()
        
        try:
            # Procura por spans com classes especÃ­ficas de valores
            value_spans = await card.query_selector_all('span')
            
            for span in value_spans:
                text = await span.text_content()
                if not text:
                    continue
                
                text = text.strip().lower()
                
                # Extrai nÃºmero (formato: "1.234,56 â‚¬" ou "1 234,56 â‚¬")
                match = re.search(r'([\d\s.]+,\d{2})\s*â‚¬', text)
                if not match:
                    continue
                
                value_str = match.group(1).replace(' ', '').replace('.', '').replace(',', '.')
                value = float(value_str)
                
                # Identifica tipo de valor pelo contexto
                parent_text = await card.evaluate('el => el.textContent')
                parent_lower = parent_text.lower()
                
                if 'base' in text or 'base' in parent_lower:
                    valores.valorBase = value
                elif 'abertura' in text or 'abertura' in parent_lower:
                    valores.valorAbertura = value
                elif 'mÃ­nimo' in text or 'minimo' in parent_lower:
                    valores.valorMinimo = value
                elif 'lance' in text or 'atual' in text or 'lance' in parent_lower:
                    valores.lanceAtual = value
                    
        except Exception as e:
            print(f"âš ï¸ Erro ao extrair valores: {e}")
        
        return valores
    
    async def _get_all_references(self, max_pages: Optional[int]) -> List[str]:
        """ObtÃ©m lista de todas as referÃªncias de eventos"""
        context = await self.browser.new_context(user_agent=self.user_agent)
        page = await context.new_page()
        
        references = []
        
        try:
            # Navega para primeira pÃ¡gina
            print("ðŸŒ Navegando para e-leiloes.pt...")
            await page.goto("https://www.e-leiloes.pt/", wait_until="networkidle")
            await asyncio.sleep(2)  # Espera mais tempo para JS carregar
            
            # Extrai total de pÃ¡ginas - MÃšLTIPLOS SELETORES
            total_pages = 1  # Default se nÃ£o encontrar paginador
            
            # Tenta diferentes seletores para o paginador
            paginator_selectors = [
                '.p-paginator-current',
                '.p-paginator .p-paginator-current',
                '[class*="paginator-current"]',
                'span.p-paginator-current',
                '.paginator-text'
            ]
            
            paginator = None
            for selector in paginator_selectors:
                paginator = await page.query_selector(selector)
                if paginator:
                    print(f"âœ… Paginador encontrado com seletor: {selector}")
                    break
            
            if paginator:
                text = await paginator.text_content()
                print(f"ðŸ“‹ Texto do paginador: '{text}'")
                
                # Tenta extrair nÃºmero total de eventos
                match = re.search(r'(\d+)', text.replace(' ', ''))
                if match:
                    # Assume que o Ãºltimo nÃºmero Ã© o total
                    numbers = re.findall(r'\d+', text.replace(' ', ''))
                    total_events = int(numbers[-1]) if numbers else 0
                    total_pages = (total_events + 11) // 12  # 12 por pÃ¡gina
                    
                    if max_pages:
                        total_pages = min(total_pages, max_pages)
                    
                    print(f"ðŸ“„ Total de eventos: {total_events}")
                    print(f"ðŸ“„ Total de pÃ¡ginas calculadas: {total_pages}")
                else:
                    print("âš ï¸ NÃ£o consegui extrair nÃºmeros do paginador")
            else:
                print("âš ï¸ Paginador nÃ£o encontrado com nenhum seletor")
                
                # Conta eventos na pÃ¡gina atual
                cards = await page.query_selector_all('.p-evento')
                print(f"ðŸ“¦ Encontrados {len(cards)} cards na pÃ¡gina atual")
            
            # Se nÃ£o encontrou paginador, tenta descobrir dinamicamente
            if total_pages == 1:
                print("ðŸ” Modo descoberta: tentando encontrar todas as pÃ¡ginas...")
                
                page_num = 1
                while True:
                    self.current_page = page_num
                    
                    # Navega para pÃ¡gina especÃ­fica
                    url = f"https://www.e-leiloes.pt/?page={page_num}"
                    await page.goto(url, wait_until="networkidle")
                    await asyncio.sleep(1.5)
                    
                    # Extrai referÃªncias desta pÃ¡gina
                    cards = await page.query_selector_all('.p-evento')
                    
                    if not cards:
                        print(f"ðŸ“„ PÃ¡gina {page_num} vazia - fim da paginaÃ§Ã£o")
                        break
                    
                    count_before = len(references)
                    
                    for card in cards:
                        ref_element = await card.query_selector('.pi-tag + span')
                        if ref_element:
                            ref = await ref_element.text_content()
                            ref_clean = ref.strip()
                            if ref_clean not in references:  # Evita duplicados
                                references.append(ref_clean)
                    
                    count_new = len(references) - count_before
                    print(f"ðŸ“„ PÃ¡gina {page_num}: +{count_new} eventos (total: {len(references)})")
                    
                    if count_new == 0:
                        print(f"ðŸ“„ Nenhum evento novo na pÃ¡gina {page_num} - fim")
                        break
                    
                    if max_pages and page_num >= max_pages:
                        print(f"ðŸ“„ Atingido limite de {max_pages} pÃ¡ginas")
                        break
                    
                    page_num += 1
                    
            else:
                # Percorre pÃ¡ginas conhecidas
                for page_num in range(1, total_pages + 1):
                    self.current_page = page_num
                    
                    # Navega para pÃ¡gina especÃ­fica
                    url = f"https://www.e-leiloes.pt/?page={page_num}"
                    await page.goto(url, wait_until="networkidle")
                    await asyncio.sleep(1.5)
                    
                    # Extrai referÃªncias desta pÃ¡gina
                    cards = await page.query_selector_all('.p-evento')
                    
                    for card in cards:
                        ref_element = await card.query_selector('.pi-tag + span')
                        if ref_element:
                            ref = await ref_element.text_content()
                            references.append(ref.strip())
                    
                    print(f"ðŸ“„ PÃ¡gina {page_num}/{total_pages}: {len(references)} eventos")
            
            return references
            
        finally:
            await page.close()
            await context.close()
    
    def get_status(self) -> ScraperStatus:
        """Retorna status atual"""
        return ScraperStatus(
            is_running=self.is_running,
            events_processed=self.events_processed,
            events_failed=self.events_failed,
            current_page=self.current_page,
            started_at=self.started_at,
            last_update=self.last_update
        )

    # ============== MULTI-STAGE SCRAPING ==============
    # Stage 1: Scrape apenas IDs
    # Stage 2: Scrape detalhes por ID
    # Stage 3: Scrape imagens por ID

    async def scrape_ids_only(self, tipo: Optional[int] = None, max_pages: Optional[int] = None) -> List[dict]:
        """
        STAGE 1: Scrape apenas referÃªncias e valores bÃ¡sicos da listagem (rÃ¡pido).

        Args:
            tipo: 1=imoveis, 2=moveis, None=ambos
            max_pages: MÃ¡ximo de pÃ¡ginas por tipo

        Returns:
            Lista de dicts: [{reference, tipo_evento, valores}, ...]
        """
        await self.init_browser()

        all_ids = []

        try:
            if tipo is None:
                # Scrape ambos os tipos
                print("ðŸ†” Stage 1: Scraping IDs de IMÃ“VEIS...")
                imoveis_ids = await self._extract_from_listing(tipo=1, max_pages=max_pages)
                for item in imoveis_ids:
                    item['tipo_evento'] = 'imovel'
                all_ids.extend(imoveis_ids)

                print("ðŸ†” Stage 1: Scraping IDs de MÃ“VEIS...")
                moveis_ids = await self._extract_from_listing(tipo=2, max_pages=max_pages)
                for item in moveis_ids:
                    item['tipo_evento'] = 'movel'
                all_ids.extend(moveis_ids)
            else:
                # Scrape tipo especÃ­fico
                tipo_nome = "IMÃ“VEIS" if tipo == 1 else "MÃ“VEIS"
                print(f"ðŸ†” Stage 1: Scraping IDs de {tipo_nome}...")
                ids = await self._extract_from_listing(tipo=tipo, max_pages=max_pages)
                tipo_evento = 'imovel' if tipo == 1 else 'movel'
                for item in ids:
                    item['tipo_evento'] = tipo_evento
                all_ids.extend(ids)

            print(f"âœ… Stage 1 completo: {len(all_ids)} IDs recolhidos")
            return all_ids

        except Exception as e:
            print(f"âŒ Erro no Stage 1: {e}")
            raise

    async def scrape_details_by_ids(self, references: List[str]) -> List[EventData]:
        """
        STAGE 2: Scrape detalhes completos (SEM imagens) para lista de referÃªncias.

        Args:
            references: Lista de referÃªncias (ex: ["LO-2024-001", "NP-2024-002"])

        Returns:
            Lista de EventData (sem imagens)
        """
        await self.init_browser()

        events = []
        failed = []

        print(f"ðŸ“‹ Stage 2: Scraping detalhes de {len(references)} eventos...")

        # Processa em batches paralelos
        for i in range(0, len(references), self.concurrent):
            batch = references[i:i + self.concurrent]

            tasks = []
            for ref in batch:
                # Determina tipo baseado no prefixo
                tipo_evento = "imovel" if ref.startswith("LO") or ref.startswith("NP") else "imovel"

                # Cria preview fake (valores virÃ£o da pÃ¡gina)
                preview = {
                    'reference': ref,
                    'valores': ValoresLeilao()
                }

                tasks.append(self._scrape_event_details_no_images(preview, tipo_evento))

            results = await asyncio.gather(*tasks, return_exceptions=True)

            for idx, result in enumerate(results):
                if isinstance(result, EventData):
                    events.append(result)
                    print(f"  âœ“ {batch[idx]}")
                else:
                    failed.append(batch[idx])
                    print(f"  âœ— {batch[idx]}: {str(result)[:50]}")

            await asyncio.sleep(self.delay)

        print(f"âœ… Stage 2 completo: {len(events)} eventos / {len(failed)} falhas")
        return events

    async def _scrape_event_details_no_images(self, preview: dict, tipo_evento: str) -> EventData:
        """
        Scrape detalhes de um evento SEM extrair imagens (mais rÃ¡pido).
        Similar a _scrape_event_details mas pula a galeria.
        """
        reference = preview['reference']
        valores_listagem = preview['valores']

        url = f"https://www.e-leiloes.pt/evento/{reference}"

        context = await self.browser.new_context(
            user_agent=self.user_agent,
            viewport={'width': 1920, 'height': 1080}
        )

        page = await context.new_page()

        try:
            await page.goto(url, wait_until="networkidle", timeout=15000)
            await asyncio.sleep(1.5)

            # Extrai datas
            data_inicio, data_fim = await self._extract_dates(page)

            # GPS (apenas imÃ³veis)
            gps = None
            if tipo_evento == "imovel":
                gps = await self._extract_gps(page)

            # Detalhes
            if tipo_evento == "imovel":
                detalhes = await self._extract_imovel_details(page)
            else:
                detalhes = await self._extract_movel_details(page)

            # Valores
            valores_pagina = await self._extract_valores_from_page(page)
            valores_final = ValoresLeilao(
                valorBase=valores_pagina.valorBase or valores_listagem.valorBase,
                valorAbertura=valores_pagina.valorAbertura or valores_listagem.valorAbertura,
                valorMinimo=valores_pagina.valorMinimo or valores_listagem.valorMinimo,
                lanceAtual=valores_pagina.lanceAtual or valores_listagem.lanceAtual
            )

            # Textos e informaÃ§Ãµes (SEM IMAGENS)
            descricao = await self._extract_descricao(page)
            observacoes = await self._extract_observacoes(page)
            descricao_predial = await self._extract_descricao_predial(page)
            cerimonia = await self._extract_cerimonia(page)
            agente = await self._extract_agente(page)
            dados_processo = await self._extract_dados_processo(page)

            return EventData(
                reference=reference,
                tipoEvento=tipo_evento,
                valores=valores_final,
                gps=gps,
                detalhes=detalhes,
                dataInicio=data_inicio,
                dataFim=data_fim,
                imagens=[],  # VAZIO - Stage 3 preenche isto
                descricao=descricao,
                observacoes=observacoes,
                descricaoPredial=descricao_predial,
                cerimoniaEncerramento=cerimonia,
                agenteExecucao=agente,
                dadosProcesso=dados_processo,
                scraped_at=datetime.utcnow()
            )

        finally:
            await page.close()
            await context.close()

    async def scrape_images_by_ids(self, references: List[str]) -> dict:
        """
        STAGE 3: Scrape apenas imagens para lista de referÃªncias.

        Args:
            references: Lista de referÃªncias

        Returns:
            Dict: {reference: [image_urls], ...}
        """
        await self.init_browser()

        images_map = {}
        failed = []

        print(f"ðŸ–¼ï¸ Stage 3: Scraping imagens de {len(references)} eventos...")

        # Processa em batches paralelos
        for i in range(0, len(references), self.concurrent):
            batch = references[i:i + self.concurrent]

            tasks = [self._scrape_images_only(ref) for ref in batch]
            results = await asyncio.gather(*tasks, return_exceptions=True)

            for idx, result in enumerate(results):
                ref = batch[idx]
                if isinstance(result, list):
                    images_map[ref] = result
                    print(f"  âœ“ {ref}: {len(result)} imagens")
                else:
                    images_map[ref] = []
                    failed.append(ref)
                    print(f"  âœ— {ref}: {str(result)[:50]}")

            await asyncio.sleep(self.delay)

        print(f"âœ… Stage 3 completo: {len(images_map)} eventos / {len(failed)} falhas")
        return images_map

    async def _scrape_images_only(self, reference: str) -> List[str]:
        """Scrape apenas as imagens de um evento"""
        url = f"https://www.e-leiloes.pt/evento/{reference}"

        context = await self.browser.new_context(
            user_agent=self.user_agent,
            viewport={'width': 1920, 'height': 1080}
        )

        page = await context.new_page()

        try:
            await page.goto(url, wait_until="networkidle", timeout=15000)
            await asyncio.sleep(1.5)

            # Extrai apenas galeria
            images = await self._extract_gallery(page)
            return images

        finally:
            await page.close()
            await context.close()
