"""
Web Scraper para E-Leiloes.pt usando Playwright
"""

import sys
import asyncio

# Fix para Windows - asyncio subprocess com Playwright
# CR√çTICO: WindowsProactorEventLoopPolicy suporta subprocessos
if sys.platform == 'win32':
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

from typing import List, Optional, Callable, Awaitable
from datetime import datetime
import re
from playwright.async_api import async_playwright, Page, Browser
import os

from models import EventData, GPSCoordinates, EventDetails, ValoresLeilao, ScraperStatus, TIPO_EVENTO_MAP, TIPO_EVENTO_NAMES, TIPO_TO_WEBSITE


class EventScraper:
    """Scraper ass√≠ncrono para e-leiloes.pt"""
    
    def __init__(self):
        self.browser: Optional[Browser] = None
        self.playwright = None
        
        # Status tracking
        self.is_running = False
        self.stop_requested = False
        self.events_processed = 0
        self.events_failed = 0
        self.current_page = None
        self.started_at = None
        self.last_update = None
        
        # Config
        self.delay = float(os.getenv("SCRAPE_DELAY", 0.8))
        self.concurrent = int(os.getenv("CONCURRENT_REQUESTS", 4))
        self.user_agent = os.getenv("USER_AGENT", "Mozilla/5.0 (Windows NT 10.0; Win64; x64)")
    
    async def init_browser(self):
        """Inicializa browser Playwright"""
        if not self.browser:
            self.playwright = await async_playwright().start()
            self.browser = await self.playwright.chromium.launch(
                headless=True,
                args=['--disable-blink-features=AutomationControlled']
            )
    
    async def close(self):
        """Fecha browser"""
        if self.browser:
            await self.browser.close()
        if self.playwright:
            await self.playwright.stop()

    def stop(self):
        """Solicita parada do scraping"""
        print("üõë Paragem do scraper solicitada...")
        self.stop_requested = True

    async def scrape_event(self, reference: str) -> EventData:
        """
        Scrape p√∫blico de um √∫nico evento por refer√™ncia.
        Detecta automaticamente se √© im√≥vel (LO) ou m√≥vel (NP) pela refer√™ncia.

        Args:
            reference: Refer√™ncia do evento (ex: LO1234567890 ou NP1234567890)

        Returns:
            EventData completo do evento
        """
        await self.init_browser()

        # Determina tipo baseado no prefixo da refer√™ncia
        # LO = Leil√£o Online (geralmente im√≥veis)
        # NP = Negocia√ß√£o Particular (pode ser m√≥veis ou im√≥veis)
        # Para seguran√ßa, vamos tentar buscar a p√°gina e detectar o tipo
        tipo_evento = "imovel" if reference.startswith("LO") else "imovel"  # default imovel

        # Cria preview fake (valores vir√£o da p√°gina individual)
        preview = {
            'reference': reference,
            'valores': ValoresLeilao()  # Vazio, ser√° preenchido na p√°gina
        }

        try:
            return await self._scrape_event_details(preview, tipo_evento)
        except Exception as e:
            raise Exception(f"Erro ao fazer scrape do evento {reference}: {str(e)}")

    async def _scrape_event_details(self, preview: dict, tipo_evento: str) -> EventData:
        """
        FASE 2: Entra na p√°gina individual para extrair detalhes completos
        
        Args:
            preview: Dict com {reference, valores} da listagem
            tipo_evento: "imovel" ou "movel"
        """
        reference = preview['reference']
        valores_listagem = preview['valores']
        
        url = f"https://www.e-leiloes.pt/evento/{reference}"
        
        context = await self.browser.new_context(
            user_agent=self.user_agent,
            viewport={'width': 1920, 'height': 1080}
        )
        
        page = await context.new_page()
        
        try:
            # Navega para p√°gina do evento
            await page.goto(url, wait_until="networkidle", timeout=15000)
            await asyncio.sleep(1.5)

            # Extrai datas do evento
            data_inicio, data_fim = await self._extract_dates(page)

            # Extrai localiza√ß√£o UMA VEZ (GPS + Distrito/Concelho/Freguesia)
            gps, distrito, concelho, freguesia = await self._extract_localizacao(page)

            # Detalhes - extrai AMBOS os tipos para dete√ß√£o autom√°tica
            detalhes_imovel = await self._extract_imovel_details(page, distrito, concelho, freguesia)
            detalhes_movel = await self._extract_movel_details(page, distrito, concelho, freguesia)

            # DETEC√á√ÉO AUTOM√ÅTICA: Se tem matr√≠cula, marca, modelo, etc ‚Üí √© m√≥vel
            is_movel = (
                detalhes_movel.matricula is not None or
                "ve√≠culo" in (detalhes_movel.tipo or "").lower() or
                "veiculo" in (detalhes_movel.tipo or "").lower() or
                "ligeiro" in (detalhes_movel.tipo or "").lower() or
                "pesado" in (detalhes_movel.tipo or "").lower() or
                "motociclo" in (detalhes_movel.tipo or "").lower()
            )

            # Define o tipo correto
            if is_movel:
                tipo_evento = "movel"
                detalhes = detalhes_movel
                gps = None  # M√≥veis n√£o t√™m GPS (s√≥ t√™m distrito/concelho/freguesia)
            else:
                tipo_evento = "imovel"
                detalhes = detalhes_imovel
                # GPS j√° foi extra√≠do acima

            # Confirma/atualiza valores na p√°gina individual (podem ser mais precisos)
            valores_pagina = await self._extract_valores_from_page(page)

            # Merge valores (prioridade: p√°gina individual > listagem)
            valores_final = ValoresLeilao(
                valorBase=valores_pagina.valorBase or valores_listagem.valorBase,
                valorAbertura=valores_pagina.valorAbertura or valores_listagem.valorAbertura,
                valorMinimo=valores_pagina.valorMinimo or valores_listagem.valorMinimo,
                lanceAtual=valores_pagina.lanceAtual or valores_listagem.lanceAtual
            )

            # Extrai todas as sec√ß√µes (HTML completo) - com safeguard para None
            imagens = await self._extract_gallery(page)
            descricao = await self._extract_descricao(page)
            observacoes = await self._extract_observacoes(page)
            onuselimitacoes = await self._extract_onus_limitacoes(page)
            descricao_predial = await self._extract_descricao_predial(page)
            cerimonia = await self._extract_cerimonia(page)
            agente = await self._extract_agente(page)
            dados_processo = await self._extract_dados_processo(page)

            # SAFEGUARD: Garante que tudo √© None se vazio
            try:
                return EventData(
                    reference=reference,
                    tipoEvento=tipo_evento,
                    valores=valores_final,
                    gps=gps,
                    detalhes=detalhes,
                    dataInicio=data_inicio,
                    dataFim=data_fim,
                    imagens=imagens,
                    descricao=descricao,
                    observacoes=observacoes,
                    onuselimitacoes=onuselimitacoes,
                    descricaoPredial=descricao_predial,
                    cerimoniaEncerramento=cerimonia,
                    agenteExecucao=agente,
                    dadosProcesso=dados_processo,
                    scraped_at=datetime.utcnow()
                )
            except Exception as e:
                print(f"‚ùå Erro valida√ß√£o EventData para {reference}: {e}")
                raise
            
        except Exception as e:
            raise Exception(f"Erro ao scrape {reference}: {str(e)}")
        
        finally:
            await page.close()
            await context.close()
    
    async def _extract_dates(self, page: Page) -> tuple[Optional[datetime], Optional[datetime]]:
        """Extrai datas de in√≠cio e fim do evento do DOM da p√°gina"""
        try:
            data_inicio = None
            data_fim = None

            # Procura por divs que contenham as datas
            # Estrutura: <div><span>In√≠cio:</span><span class="font-semibold">DATA</span></div>
            divs = await page.query_selector_all('div.flex.justify-content-between')

            for div in divs:
                text = await div.text_content()
                if not text:
                    continue

                text = text.strip()

                # Verifica se √© a div de In√≠cio
                if 'In√≠cio:' in text:
                    # Busca o span com font-semibold dentro desta div
                    date_span = await div.query_selector('span.font-semibold')
                    if date_span:
                        value = await date_span.text_content()
                        if value:
                            try:
                                # Parse data no formato DD/MM/YYYY HH:MM:SS
                                data_inicio = datetime.strptime(value.strip(), '%d/%m/%Y %H:%M:%S')
                            except ValueError as e:
                                print(f"‚ö†Ô∏è Erro ao parsear data de in√≠cio '{value}': {e}")

                # Verifica se √© a div de Fim
                elif 'Fim:' in text:
                    # Busca o span com font-semibold dentro desta div
                    date_span = await div.query_selector('span.font-semibold')
                    if date_span:
                        value = await date_span.text_content()
                        if value:
                            try:
                                # Parse data no formato DD/MM/YYYY HH:MM:SS
                                data_fim = datetime.strptime(value.strip(), '%d/%m/%Y %H:%M:%S')
                            except ValueError as e:
                                print(f"‚ö†Ô∏è Erro ao parsear data de fim '{value}': {e}")

            return data_inicio, data_fim

        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao extrair datas: {e}")
            return None, None

    async def _extract_localizacao(self, page: Page) -> tuple[GPSCoordinates, Optional[str], Optional[str], Optional[str]]:
        """
        Extrai dados da sec√ß√£o Localiza√ß√£o: GPS, Distrito, Concelho, Freguesia.
        Funciona para m√≥veis e im√≥veis.

        Returns:
            (gps, distrito, concelho, freguesia)
        """
        try:
            latitude = None
            longitude = None
            distrito = None
            concelho = None
            freguesia = None

            # Primeiro, encontra a div com t√≠tulo "Localiza√ß√£o"
            localizacao_section = None
            title_divs = await page.query_selector_all('.font-semibold.text-xl')

            for title_div in title_divs:
                text = await title_div.text_content()
                if text and 'Localiza√ß√£o' in text.strip():
                    # Pega o elemento pai mais pr√≥ximo (bg-white shadow-1)
                    localizacao_section = await title_div.evaluate_handle(
                        'el => el.closest(".bg-white.shadow-1") || el.closest(".flex.flex-column.w-full")'
                    )
                    break

            if not localizacao_section:
                print("‚ö†Ô∏è Se√ß√£o 'Localiza√ß√£o' n√£o encontrada")
                return GPSCoordinates(latitude=None, longitude=None), None, None, None

            # Procura por todos os containers .flex.flex-wrap.gap-1
            field_containers = await localizacao_section.query_selector_all('.flex.flex-wrap.gap-1')

            for container in field_containers:
                # Dentro de cada container, procura label e valor
                spans = await container.query_selector_all('span')
                if len(spans) >= 2:
                    label_span = spans[0]
                    value_span = spans[1]

                    label = await label_span.text_content()
                    value = await value_span.text_content()

                    if not label or not value:
                        continue

                    label = label.strip()
                    value = value.strip()

                    # GPS
                    if 'GPS Latitude' in label:
                        try:
                            latitude = float(value)
                            print(f"  ‚úì GPS Lat: {latitude}")
                        except ValueError:
                            pass

                    elif 'GPS Longitude' in label:
                        try:
                            longitude = float(value)
                            print(f"  ‚úì GPS Lon: {longitude}")
                        except ValueError:
                            pass

                    # Distrito / Concelho / Freguesia
                    elif 'Distrito' in label:
                        distrito = value
                        print(f"  ‚úì Distrito: {distrito}")

                    elif 'Concelho' in label:
                        concelho = value
                        print(f"  ‚úì Concelho: {concelho}")

                    elif 'Freguesia' in label:
                        freguesia = value
                        print(f"  ‚úì Freguesia: {freguesia}")

            gps = GPSCoordinates(latitude=latitude, longitude=longitude)
            return gps, distrito, concelho, freguesia

        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao extrair Localiza√ß√£o: {e}")
            import traceback
            traceback.print_exc()
            return GPSCoordinates(latitude=None, longitude=None), None, None, None

    async def _extract_gps(self, page: Page) -> GPSCoordinates:
        """Extrai coordenadas GPS (wrapper para compatibilidade)"""
        gps, _, _, _ = await self._extract_localizacao(page)
        return gps

    async def _extract_gallery(self, page: Page) -> List[str]:
        """Extrai TODAS as imagens iterando pelos IDs pv_id_X_item_Y"""
        try:
            import re

            # Aguarda a galeria carregar
            try:
                await page.wait_for_selector('.p-galleria', timeout=3000)
            except:
                print("‚ö†Ô∏è Galeria n√£o encontrada")
                return []

            # ===== NOVO: Aguarda pelo contador de imagens e calcula tempo de espera =====
            try:
                # Tenta encontrar o contador de imagens (ex: "1/7")
                footer_selector = '.custom-galleria-footer, .p-galleria-footer, .better-image-badge'
                await page.wait_for_selector(footer_selector, timeout=2000)

                footer = await page.query_selector(footer_selector)
                if footer:
                    footer_text = await footer.inner_text()
                    print(f"üìä Contador de imagens: {footer_text}")

                    # Parse "X/Y" ou "üì∑ Y" para obter total de imagens
                    match = re.search(r'(\d+)/(\d+)|üì∑\s*(\d+)', footer_text)
                    if match:
                        total_images = int(match.group(2) if match.group(2) else match.group(3))
                        print(f"üñºÔ∏è Total de imagens detectadas: {total_images}")

                        # Calcula tempo de espera: 2.5s por imagem + 1s buffer
                        wait_time = (total_images * 2.5) + 1
                        print(f"‚è≥ Aguardando {wait_time:.1f}s para todas as imagens carregarem...")
                        await asyncio.sleep(wait_time)
                    else:
                        # Fallback: se n√£o conseguiu parsear, espera 3s
                        print("‚ö†Ô∏è N√£o conseguiu parsear contador, aguardando 3s...")
                        await asyncio.sleep(3)
                else:
                    # Se n√£o encontrou footer, espera tempo padr√£o
                    print("‚ö†Ô∏è Footer n√£o encontrado, aguardando 2s...")
                    await asyncio.sleep(2)
            except Exception as e:
                print(f"‚ö†Ô∏è Erro ao detectar contador: {e}, aguardando 2s...")
                await asyncio.sleep(2)

            images = []

            # M√©todo 1: Itera pelos items da galeria (pv_id_X_item_0, item_1, item_2...)
            try:
                # Encontra o primeiro item para obter o prefixo do ID
                first_item = await page.query_selector('.p-galleria-item[id]')
                if first_item:
                    first_id = await first_item.get_attribute('id')
                    if first_id:
                        # Extrai prefixo (ex: "pv_id_7" de "pv_id_7_item_0")
                        id_match = re.match(r'(pv_id_\d+)_item_\d+', first_id)
                        if id_match:
                            id_prefix = id_match.group(1)
                            print(f"üîç Galeria ID: {id_prefix}")

                            # Itera de item_0 at√© n√£o encontrar mais
                            item_num = 0
                            consecutive_misses = 0
                            max_misses = 3  # Para se houver gaps

                            while consecutive_misses < max_misses and item_num < 100:
                                item_id = f"{id_prefix}_item_{item_num}"
                                item = await page.query_selector(f'#{item_id} .p-evento-image')

                                if item:
                                    style = await item.get_attribute('style')
                                    if style and 'background-image: url(' in style:
                                        match = re.search(r'url\(["\']?([^"\']+)["\']?\)', style)
                                        if match:
                                            url = match.group(1).replace('&quot;', '')
                                            if url and url not in images:
                                                images.append(url)
                                                print(f"  ‚úì Imagem {item_num}: {url.split('/')[-1]}")
                                    consecutive_misses = 0
                                else:
                                    consecutive_misses += 1

                                item_num += 1

                            print(f"üì∑ Total: {len(images)} imagens extra√≠das")
            except Exception as e:
                print(f"‚ö†Ô∏è Erro ao iterar items: {e}")

            # Fallback: Se n√£o encontrou nada, tenta buscar todos os items vis√≠veis
            if not images:
                try:
                    gallery_items = await page.query_selector_all('.p-galleria-item .p-evento-image')
                    for item in gallery_items:
                        style = await item.get_attribute('style')
                        if style and 'background-image: url(' in style:
                            match = re.search(r'url\(["\']?([^"\']+)["\']?\)', style)
                            if match:
                                url = match.group(1).replace('&quot;', '')
                                if url and url not in images:
                                    images.append(url)
                    print(f"üì∑ Fallback: {len(images)} imagens")
                except Exception as e:
                    print(f"‚ö†Ô∏è Erro no fallback: {e}")

            return images

        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao extrair galeria: {e}")
            return []

    async def _extract_section_html(self, page: Page, section_title: str) -> Optional[str]:
        """
        M√©todo gen√©rico para extrair HTML completo de uma sec√ß√£o.

        Args:
            page: P√°gina do Playwright
            section_title: T√≠tulo da sec√ß√£o (ex: "Descri√ß√£o", "Observa√ß√µes", etc.)

        Returns:
            HTML completo da sec√ß√£o ou None se n√£o encontrada ou vazia
        """
        try:
            title_divs = await page.query_selector_all('.font-semibold.text-xl')

            for title_div in title_divs:
                text = await title_div.text_content()
                if text and section_title.lower() in text.strip().lower():
                    # Pega o elemento pai (a se√ß√£o completa)
                    section = await title_div.evaluate_handle(
                        'el => el.closest(".flex.flex-column.w-full")'
                    )
                    if section:
                        # Retorna o HTML completo da sec√ß√£o
                        html = await section.evaluate('el => el.innerHTML')
                        # SAFEGUARD: Retorna None se vazio ou s√≥ whitespace
                        if html and html.strip():
                            return html.strip()
                        return None

            return None

        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao extrair sec√ß√£o '{section_title}': {e}")
            return None

    async def _extract_descricao(self, page: Page) -> Optional[str]:
        """Extrai HTML completo da sec√ß√£o Descri√ß√£o"""
        return await self._extract_section_html(page, "Descri√ß√£o")

    async def _extract_observacoes(self, page: Page) -> Optional[str]:
        """Extrai HTML completo da sec√ß√£o Observa√ß√µes"""
        return await self._extract_section_html(page, "Observa√ß√µes")

    async def _extract_onus_limitacoes(self, page: Page) -> Optional[str]:
        """Extrai HTML completo da sec√ß√£o √ìnus e Limita√ß√µes"""
        return await self._extract_section_html(page, "√ìnus")

    async def _extract_descricao_predial(self, page: Page) -> Optional[str]:
        """Extrai HTML completo da sec√ß√£o Descri√ß√£o Predial"""
        return await self._extract_section_html(page, "Descri√ß√£o Predial")

    async def _extract_cerimonia(self, page: Page) -> Optional[str]:
        """Extrai HTML completo da sec√ß√£o Cerim√≥nia de Encerramento"""
        return await self._extract_section_html(page, "Cerim√≥nia")

    async def _extract_agente(self, page: Page) -> Optional[str]:
        """Extrai HTML completo da sec√ß√£o Agente de Execu√ß√£o"""
        return await self._extract_section_html(page, "Agente")

    async def _extract_dados_processo(self, page: Page) -> Optional[str]:
        """Extrai HTML completo da sec√ß√£o Dados do Processo"""
        return await self._extract_section_html(page, "Dados do Processo")

    async def _extract_descricao_predial_OLD(self, page: Page):
        """Extrai informa√ß√£o da descri√ß√£o predial"""
        try:
            from models import DescricaoPredial

            # Procura pela se√ß√£o "Descri√ß√£o Predial"
            title_divs = await page.query_selector_all('.font-semibold.text-xl')

            for title_div in title_divs:
                text = await title_div.text_content()
                if text and 'Descri√ß√£o Predial' in text.strip():
                    section = await title_div.evaluate_handle(
                        'el => el.closest(".flex.flex-column.w-full")'
                    )
                    if not section:
                        continue

                    # Extrai campos
                    numero_desc = None
                    fracao = None
                    distrito_code = None
                    concelho_code = None
                    freguesia_code = None
                    artigos = []

                    spans = await section.query_selector_all('.font-semibold')
                    for span in spans:
                        label = await span.text_content()
                        if not label:
                            continue

                        label = label.strip()

                        # Pega o pr√≥ximo elemento (o valor)
                        next_el = await span.evaluate_handle('el => el.nextElementSibling')
                        if next_el:
                            value = await next_el.text_content()
                            if value:
                                value = value.strip()

                                if 'N.¬∫ da Descri√ß√£o:' in label:
                                    numero_desc = value
                                elif 'Fra√ß√£o:' in label:
                                    fracao = value if value else None
                                elif 'Distrito:' in label:
                                    distrito_code = value
                                elif 'Concelho:' in label:
                                    concelho_code = value
                                elif 'Freguesia:' in label:
                                    freguesia_code = value

                    return DescricaoPredial(
                        numeroDescricao=numero_desc,
                        fracao=fracao,
                        distritoCode=distrito_code,
                        concelhoCode=concelho_code,
                        freguesiaCode=freguesia_code,
                        artigos=artigos
                    )

            return None

        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao extrair descri√ß√£o predial: {e}")
            return None


    async def _extract_valores_from_page(self, page: Page) -> ValoresLeilao:
        """Extrai valores da p√°gina individual do evento"""
        valores = ValoresLeilao()
        
        try:
            # Procura por elementos com valores
            body_text = await page.text_content('body')
            
            # Regex para valores monet√°rios
            value_patterns = {
                'valorBase': r'(?:valor\s+base|base)[:\s]*‚Ç¨?\s*([\d\s.]+,\d{2})',
                'valorAbertura': r'(?:valor\s+abertura|abertura)[:\s]*‚Ç¨?\s*([\d\s.]+,\d{2})',
                'valorMinimo': r'(?:valor\s+m[i√≠]nimo|m[i√≠]nimo)[:\s]*‚Ç¨?\s*([\d\s.]+,\d{2})',
                'lanceAtual': r'(?:lance\s+atual|atual)[:\s]*‚Ç¨?\s*([\d\s.]+,\d{2})'
            }
            
            for field, pattern in value_patterns.items():
                match = re.search(pattern, body_text, re.IGNORECASE)
                if match:
                    value_str = match.group(1).replace(' ', '').replace('.', '').replace(',', '.')
                    setattr(valores, field, float(value_str))
                    
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao extrair valores da p√°gina: {e}")
        
        return valores
    
    async def _extract_imovel_details(
        self,
        page: Page,
        distrito: Optional[str] = None,
        concelho: Optional[str] = None,
        freguesia: Optional[str] = None
    ) -> EventDetails:
        """Extrai detalhes COMPLETOS do IM√ìVEL via DOM"""

        async def extract_detail(label: str) -> Optional[str]:
            """Extrai valor de um campo espec√≠fico"""
            try:
                # Procura por span.font-semibold com o label
                spans = await page.query_selector_all('.flex.w-full .font-semibold')

                for span in spans:
                    text = await span.text_content()
                    if text and text.strip() == label:
                        # Pega pr√≥ximo elemento
                        next_el = await span.evaluate_handle('el => el.nextElementSibling')
                        if next_el:
                            value = await next_el.text_content()
                            return value.strip() if value else None

                return None
            except:
                return None

        async def extract_area(label: str) -> Optional[float]:
            """Extrai √°rea em m¬≤"""
            try:
                spans = await page.query_selector_all('.flex.w-full .font-semibold')

                for span in spans:
                    text = await span.text_content()
                    if text and text.strip() == label:
                        # Procura por span.mr-1 no wrapper
                        wrapper = await span.evaluate_handle('el => el.closest(".flex.w-full")')
                        if wrapper:
                            number_span = await wrapper.query_selector('span.mr-1')
                            if number_span:
                                value = await number_span.text_content()
                                # Remove espa√ßos e converte para float
                                return float(value.replace(',', '.').strip())

                return None
            except:
                return None

        # Extrai campos b√°sicos
        tipo = await extract_detail('Tipo:')
        subtipo = await extract_detail('Subtipo:')
        tipologia = await extract_detail('Tipologia:')

        area_priv = await extract_area('√Årea Privativa:')
        area_dep = await extract_area('√Årea Dependente:')
        area_total = await extract_area('√Årea Total:')

        return EventDetails(
            tipo=tipo,
            subtipo=subtipo,
            tipologia=tipologia,
            areaPrivativa=area_priv,
            areaDependente=area_dep,
            areaTotal=area_total,
            distrito=distrito,
            concelho=concelho,
            freguesia=freguesia
        )
    
    async def _extract_movel_details(
        self,
        page: Page,
        distrito: Optional[str] = None,
        concelho: Optional[str] = None,
        freguesia: Optional[str] = None
    ) -> EventDetails:
        """Extrai detalhes do M√ìVEL (autom√≥vel) via DOM"""

        async def extract_detail(label: str) -> Optional[str]:
            """Extrai valor de um campo espec√≠fico"""
            try:
                spans = await page.query_selector_all('.flex.w-full .font-semibold')

                for span in spans:
                    text = await span.text_content()
                    if text and text.strip() == label:
                        next_el = await span.evaluate_handle('el => el.nextElementSibling')
                        if next_el:
                            value = await next_el.text_content()
                            return value.strip() if value else None

                return None
            except:
                return None

        # Extrai campos b√°sicos
        tipo = await extract_detail('Tipo:')
        subtipo = await extract_detail('Subtipo:')
        matricula = await extract_detail('Matr√≠cula:')

        return EventDetails(
            tipo=tipo,
            subtipo=subtipo,
            matricula=matricula,
            distrito=distrito,
            concelho=concelho,
            freguesia=freguesia
        )
    
    async def scrape_all_events(self, max_pages: Optional[int] = None) -> List[EventData]:
        """
        Scrape TODOS os eventos (IMOVEIS + MOVEIS) do site

        Args:
            max_pages: M√°ximo de p√°ginas para processar POR TIPO (None = todas)

        Returns:
            Lista com todos os eventos
        """
        self.is_running = True
        self.stop_requested = False
        self.started_at = datetime.utcnow()
        self.events_processed = 0
        self.events_failed = 0

        all_events = []

        await self.init_browser()

        try:
            # 1. SCRAPE IMOVEIS (tipo=1)
            if not self.stop_requested:
                print("üè† Iniciando scraping de IM√ìVEIS...")
                imoveis = await self._scrape_by_type(tipo=1, max_pages=max_pages)
                all_events.extend(imoveis)
                print(f"‚úÖ Im√≥veis recolhidos: {len(imoveis)}")

            # 2. SCRAPE MOVEIS (tipo=2)
            if not self.stop_requested:
                print("üöó Iniciando scraping de M√ìVEIS...")
                moveis = await self._scrape_by_type(tipo=2, max_pages=max_pages)
                all_events.extend(moveis)
                print(f"‚úÖ M√≥veis recolhidos: {len(moveis)}")

            if self.stop_requested:
                print(f"‚ö†Ô∏è Scraping interrompido pelo utilizador. Total processado: {len(all_events)}")
            else:
                print(f"üéâ Total de eventos: {len(all_events)}")

            return all_events

        finally:
            self.is_running = False
            self.stop_requested = False
    
    async def _scrape_by_type(self, tipo: int, max_pages: Optional[int] = None) -> List[EventData]:
        """
        Scrape eventos de um tipo espec√≠fico (1=imovel, 2=movel)
        
        FASE 1: Extrai refer√™ncias + valores da listagem
        FASE 2: Entra em cada evento para detalhes + GPS
        """
        tipo_nome = "imovel" if tipo == 1 else "movel"
        
        # FASE 1: P√°gina de listagem
        events_preview = await self._extract_from_listing(tipo, max_pages)
        print(f"üìã {len(events_preview)} eventos {tipo_nome} encontrados na listagem")
        
        # FASE 2: P√°gina individual (paralelo)
        all_events = []

        for i in range(0, len(events_preview), self.concurrent):
            # Verifica se foi solicitada paragem
            if self.stop_requested:
                print(f"üõë Scraping interrompido na p√°gina {self.current_page}")
                break

            batch = events_preview[i:i + self.concurrent]

            tasks = [
                self._scrape_event_details(preview, tipo_nome)
                for preview in batch
            ]
            results = await asyncio.gather(*tasks, return_exceptions=True)

            for result in results:
                if isinstance(result, EventData):
                    all_events.append(result)
                    self.events_processed += 1
                    self.last_update = datetime.utcnow()
                else:
                    self.events_failed += 1
                    print(f"‚ö†Ô∏è Erro: {result}")

            print(f"üìä Processados: {self.events_processed} eventos {tipo_nome}")
            await asyncio.sleep(self.delay)

        return all_events
    
    async def _extract_from_listing(self, tipo: int, max_pages: Optional[int]) -> List[dict]:
        """
        FASE 1: Extrai refer√™ncias + valores da p√°gina de listagem
        
        Usa pagina√ß√£o com first=0, first=12, first=24, etc. (12 eventos por p√°gina)
        
        Returns:
            Lista de dicts com {reference, valores}
        """
        context = await self.browser.new_context(user_agent=self.user_agent)
        page = await context.new_page()
        
        events_preview = []
        
        try:
            page_num = 0  # Come√ßa em 0
            first_offset = 0  # Offset inicial
            
            while True:
                # Verifica se foi solicitada paragem
                if self.stop_requested:
                    print(f"üõë Scraping da listagem interrompido")
                    break

                self.current_page = page_num + 1  # Para display (p√°gina 1, 2, 3...)

                # Navega para p√°gina de listagem com offset correto
                # Converte tipo interno para tipo do website
                website_tipo = TIPO_TO_WEBSITE.get(tipo, tipo)
                url = f"https://www.e-leiloes.pt/eventos?layout=grid&first={first_offset}&sort=dataFimAsc&tipo={website_tipo}"
                print(f"üåê Navegando para p√°gina {page_num + 1} (first={first_offset})...")
                await page.goto(url, wait_until="networkidle")
                await asyncio.sleep(1.5)
                
                # Extrai cards
                cards = await page.query_selector_all('.p-evento')
                
                if not cards:
                    print(f"üìÑ P√°gina {page_num} vazia - fim")
                    break
                
                count_before = len(events_preview)
                
                for card in cards:
                    try:
                        # Refer√™ncia
                        ref_el = await card.query_selector('.pi-tag + span')
                        if not ref_el:
                            continue
                        
                        reference = (await ref_el.text_content()).strip()
                        
                        # Verifica duplicado
                        if any(e['reference'] == reference for e in events_preview):
                            continue
                        
                        # Extrai valores
                        valores = await self._extract_valores_from_card(card)
                        
                        events_preview.append({
                            'reference': reference,
                            'valores': valores
                        })
                        
                    except Exception as e:
                        print(f"‚ö†Ô∏è Erro ao extrair card: {e}")
                        continue
                
                count_new = len(events_preview) - count_before
                print(f"üìÑ P√°gina {page_num + 1}: +{count_new} eventos (total: {len(events_preview)})")
                
                if count_new == 0:
                    break
                
                if max_pages and (page_num + 1) >= max_pages:
                    print(f"üìÑ Limite de {max_pages} p√°ginas atingido")
                    break
                
                # Incrementa offset de 12 em 12
                page_num += 1
                first_offset += 12
            
            return events_preview
            
        finally:
            await page.close()
            await context.close()
    
    async def _extract_valores_from_card(self, card) -> ValoresLeilao:
        """Extrai valores (Base, Abertura, M√≠nimo, Lance Atual) de um card"""
        valores = ValoresLeilao()
        
        try:
            # Procura por spans com classes espec√≠ficas de valores
            value_spans = await card.query_selector_all('span')
            
            for span in value_spans:
                text = await span.text_content()
                if not text:
                    continue
                
                text = text.strip().lower()
                
                # Extrai n√∫mero (formato: "1.234,56 ‚Ç¨" ou "1 234,56 ‚Ç¨")
                match = re.search(r'([\d\s.]+,\d{2})\s*‚Ç¨', text)
                if not match:
                    continue
                
                value_str = match.group(1).replace(' ', '').replace('.', '').replace(',', '.')
                value = float(value_str)
                
                # Identifica tipo de valor pelo contexto
                parent_text = await card.evaluate('el => el.textContent')
                parent_lower = parent_text.lower()
                
                if 'base' in text or 'base' in parent_lower:
                    valores.valorBase = value
                elif 'abertura' in text or 'abertura' in parent_lower:
                    valores.valorAbertura = value
                elif 'm√≠nimo' in text or 'minimo' in parent_lower:
                    valores.valorMinimo = value
                elif 'lance' in text or 'atual' in text or 'lance' in parent_lower:
                    valores.lanceAtual = value
                    
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao extrair valores: {e}")
        
        return valores
    
    async def _get_all_references(self, max_pages: Optional[int]) -> List[str]:
        """Obt√©m lista de todas as refer√™ncias de eventos"""
        context = await self.browser.new_context(user_agent=self.user_agent)
        page = await context.new_page()
        
        references = []
        
        try:
            # Navega para primeira p√°gina
            print("üåê Navegando para e-leiloes.pt...")
            await page.goto("https://www.e-leiloes.pt/", wait_until="networkidle")
            await asyncio.sleep(2)  # Espera mais tempo para JS carregar
            
            # Extrai total de p√°ginas - M√öLTIPLOS SELETORES
            total_pages = 1  # Default se n√£o encontrar paginador
            
            # Tenta diferentes seletores para o paginador
            paginator_selectors = [
                '.p-paginator-current',
                '.p-paginator .p-paginator-current',
                '[class*="paginator-current"]',
                'span.p-paginator-current',
                '.paginator-text'
            ]
            
            paginator = None
            for selector in paginator_selectors:
                paginator = await page.query_selector(selector)
                if paginator:
                    print(f"‚úÖ Paginador encontrado com seletor: {selector}")
                    break
            
            if paginator:
                text = await paginator.text_content()
                print(f"üìã Texto do paginador: '{text}'")
                
                # Tenta extrair n√∫mero total de eventos
                match = re.search(r'(\d+)', text.replace(' ', ''))
                if match:
                    # Assume que o √∫ltimo n√∫mero √© o total
                    numbers = re.findall(r'\d+', text.replace(' ', ''))
                    total_events = int(numbers[-1]) if numbers else 0
                    total_pages = (total_events + 11) // 12  # 12 por p√°gina
                    
                    if max_pages:
                        total_pages = min(total_pages, max_pages)
                    
                    print(f"üìÑ Total de eventos: {total_events}")
                    print(f"üìÑ Total de p√°ginas calculadas: {total_pages}")
                else:
                    print("‚ö†Ô∏è N√£o consegui extrair n√∫meros do paginador")
            else:
                print("‚ö†Ô∏è Paginador n√£o encontrado com nenhum seletor")
                
                # Conta eventos na p√°gina atual
                cards = await page.query_selector_all('.p-evento')
                print(f"üì¶ Encontrados {len(cards)} cards na p√°gina atual")
            
            # Se n√£o encontrou paginador, tenta descobrir dinamicamente
            if total_pages == 1:
                print("üîç Modo descoberta: tentando encontrar todas as p√°ginas...")
                
                page_num = 1
                while True:
                    self.current_page = page_num
                    
                    # Navega para p√°gina espec√≠fica
                    url = f"https://www.e-leiloes.pt/?page={page_num}"
                    await page.goto(url, wait_until="networkidle")
                    await asyncio.sleep(1.5)
                    
                    # Extrai refer√™ncias desta p√°gina
                    cards = await page.query_selector_all('.p-evento')
                    
                    if not cards:
                        print(f"üìÑ P√°gina {page_num} vazia - fim da pagina√ß√£o")
                        break
                    
                    count_before = len(references)
                    
                    for card in cards:
                        ref_element = await card.query_selector('.pi-tag + span')
                        if ref_element:
                            ref = await ref_element.text_content()
                            ref_clean = ref.strip()
                            if ref_clean not in references:  # Evita duplicados
                                references.append(ref_clean)
                    
                    count_new = len(references) - count_before
                    print(f"üìÑ P√°gina {page_num}: +{count_new} eventos (total: {len(references)})")
                    
                    if count_new == 0:
                        print(f"üìÑ Nenhum evento novo na p√°gina {page_num} - fim")
                        break
                    
                    if max_pages and page_num >= max_pages:
                        print(f"üìÑ Atingido limite de {max_pages} p√°ginas")
                        break
                    
                    page_num += 1
                    
            else:
                # Percorre p√°ginas conhecidas
                for page_num in range(1, total_pages + 1):
                    self.current_page = page_num
                    
                    # Navega para p√°gina espec√≠fica
                    url = f"https://www.e-leiloes.pt/?page={page_num}"
                    await page.goto(url, wait_until="networkidle")
                    await asyncio.sleep(1.5)
                    
                    # Extrai refer√™ncias desta p√°gina
                    cards = await page.query_selector_all('.p-evento')
                    
                    for card in cards:
                        ref_element = await card.query_selector('.pi-tag + span')
                        if ref_element:
                            ref = await ref_element.text_content()
                            references.append(ref.strip())
                    
                    print(f"üìÑ P√°gina {page_num}/{total_pages}: {len(references)} eventos")
            
            return references
            
        finally:
            await page.close()
            await context.close()
    
    def get_status(self) -> ScraperStatus:
        """Retorna status atual"""
        return ScraperStatus(
            is_running=self.is_running,
            events_processed=self.events_processed,
            events_failed=self.events_failed,
            current_page=self.current_page,
            started_at=self.started_at,
            last_update=self.last_update
        )

    # ============== MULTI-STAGE SCRAPING ==============
    # Stage 1: Scrape apenas IDs
    # Stage 2: Scrape detalhes por ID
    # Stage 3: Scrape imagens por ID

    async def scrape_ids_only(self, tipo: Optional[int] = None, max_pages: Optional[int] = None) -> List[dict]:
        """
        STAGE 1: Scrape apenas refer√™ncias e valores b√°sicos da listagem (r√°pido).

        Args:
            tipo: 1-6 para tipo espec√≠fico, None=todos os tipos
                  1=Im√≥veis, 2=Ve√≠culos, 3=Direitos, 4=Equipamentos, 5=Mobili√°rio, 6=M√°quinas
            max_pages: M√°ximo de p√°ginas por tipo

        Returns:
            Lista de dicts: [{reference, tipo_evento, valores}, ...]
        """
        await self.init_browser()

        all_ids = []

        try:
            if tipo is None:
                # Scrape TODOS os 6 tipos
                for tipo_code, tipo_str in TIPO_EVENTO_MAP.items():
                    tipo_nome = TIPO_EVENTO_NAMES[tipo_code]
                    print(f"üÜî Stage 1: Scraping IDs de {tipo_nome} (tipo={tipo_code})...")
                    ids = await self._extract_from_listing(tipo=tipo_code, max_pages=max_pages)
                    for item in ids:
                        item['tipo_evento'] = tipo_str
                        item['tipo'] = tipo_str  # Alias para compatibilidade
                    all_ids.extend(ids)
                    print(f"  ‚úì {len(ids)} {tipo_nome} encontrados")
            else:
                # Scrape tipo espec√≠fico
                if tipo not in TIPO_EVENTO_MAP:
                    raise ValueError(f"Tipo inv√°lido: {tipo}. Use 1-6.")
                tipo_str = TIPO_EVENTO_MAP[tipo]
                tipo_nome = TIPO_EVENTO_NAMES[tipo]
                print(f"üÜî Stage 1: Scraping IDs de {tipo_nome} (tipo={tipo})...")
                ids = await self._extract_from_listing(tipo=tipo, max_pages=max_pages)
                for item in ids:
                    item['tipo_evento'] = tipo_str
                    item['tipo'] = tipo_str
                all_ids.extend(ids)

            print(f"‚úÖ Stage 1 completo: {len(all_ids)} IDs recolhidos")
            return all_ids

        except Exception as e:
            print(f"‚ùå Erro no Stage 1: {e}")
            raise

    async def scrape_details_by_ids(
        self,
        references: List[str],
        on_event_scraped: Optional[Callable[[EventData], Awaitable[None]]] = None
    ) -> List[EventData]:
        """
        STAGE 2: Scrape detalhes completos (SEM imagens) para lista de refer√™ncias.

        Args:
            references: Lista de refer√™ncias (ex: ["LO-2024-001", "NP-2024-002"])
            on_event_scraped: Callback async chamado para cada evento scraped (inser√ß√£o em tempo real)

        Returns:
            Lista de EventData (sem imagens)
        """
        await self.init_browser()

        events = []
        failed = []

        print(f"üìã Stage 2: Scraping detalhes de {len(references)} eventos...")

        # Processa em batches paralelos
        for i in range(0, len(references), self.concurrent):
            batch = references[i:i + self.concurrent]

            tasks = []
            for ref in batch:
                # Determina tipo baseado no prefixo
                tipo_evento = "imovel" if ref.startswith("LO") or ref.startswith("NP") else "imovel"

                # Cria preview fake (valores vir√£o da p√°gina)
                preview = {
                    'reference': ref,
                    'valores': ValoresLeilao()
                }

                tasks.append(self._scrape_event_details_no_images(preview, tipo_evento))

            results = await asyncio.gather(*tasks, return_exceptions=True)

            for idx, result in enumerate(results):
                if isinstance(result, EventData):
                    events.append(result)
                    print(f"  ‚úì {batch[idx]}")

                    # üî• INSER√á√ÉO EM TEMPO REAL via callback
                    if on_event_scraped:
                        try:
                            await on_event_scraped(result)
                        except Exception as e:
                            print(f"  ‚ö†Ô∏è Erro ao salvar {batch[idx]}: {e}")
                else:
                    failed.append(batch[idx])
                    print(f"  ‚úó {batch[idx]}: {str(result)[:50]}")

            await asyncio.sleep(self.delay)

        print(f"‚úÖ Stage 2 completo: {len(events)} eventos / {len(failed)} falhas")
        return events

    async def _scrape_event_details_no_images(self, preview: dict, tipo_evento: str) -> EventData:
        """
        Scrape detalhes de um evento SEM extrair imagens (mais r√°pido).
        Similar a _scrape_event_details mas pula a galeria.
        """
        reference = preview['reference']
        valores_listagem = preview['valores']

        url = f"https://www.e-leiloes.pt/evento/{reference}"

        context = await self.browser.new_context(
            user_agent=self.user_agent,
            viewport={'width': 1920, 'height': 1080}
        )

        page = await context.new_page()

        try:
            await page.goto(url, wait_until="networkidle", timeout=15000)
            await asyncio.sleep(1.5)

            # Extrai datas
            data_inicio, data_fim = await self._extract_dates(page)

            # Extrai localiza√ß√£o UMA VEZ (GPS + Distrito/Concelho/Freguesia)
            gps, distrito, concelho, freguesia = await self._extract_localizacao(page)

            # Detalhes - extrai AMBOS os tipos para detetar automaticamente
            detalhes_imovel = await self._extract_imovel_details(page, distrito, concelho, freguesia)
            detalhes_movel = await self._extract_movel_details(page, distrito, concelho, freguesia)

            # DETEC√á√ÉO AUTOM√ÅTICA: Se tem matr√≠cula, marca, modelo, etc ‚Üí √© m√≥vel
            # Se tem tipologia, √°rea, distrito, etc ‚Üí √© im√≥vel
            is_movel = (
                detalhes_movel.matricula is not None or
                "ve√≠culo" in (detalhes_movel.tipo or "").lower() or
                "veiculo" in (detalhes_movel.tipo or "").lower() or
                "ligeiro" in (detalhes_movel.tipo or "").lower() or
                "pesado" in (detalhes_movel.tipo or "").lower() or
                "motociclo" in (detalhes_movel.tipo or "").lower()
            )

            # Define o tipo correto
            if is_movel:
                tipo_evento = "movel"
                detalhes = detalhes_movel
                gps = None  # M√≥veis n√£o t√™m GPS (s√≥ t√™m distrito/concelho/freguesia)
            else:
                tipo_evento = "imovel"
                detalhes = detalhes_imovel
                # GPS j√° foi extra√≠do acima

            # Valores
            valores_pagina = await self._extract_valores_from_page(page)
            valores_final = ValoresLeilao(
                valorBase=valores_pagina.valorBase or valores_listagem.valorBase,
                valorAbertura=valores_pagina.valorAbertura or valores_listagem.valorAbertura,
                valorMinimo=valores_pagina.valorMinimo or valores_listagem.valorMinimo,
                lanceAtual=valores_pagina.lanceAtual or valores_listagem.lanceAtual
            )

            # Textos e informa√ß√µes (SEM IMAGENS) - com safeguard para None
            descricao = await self._extract_descricao(page)
            observacoes = await self._extract_observacoes(page)
            onuselimitacoes = await self._extract_onus_limitacoes(page)
            descricao_predial = await self._extract_descricao_predial(page)
            cerimonia = await self._extract_cerimonia(page)
            agente = await self._extract_agente(page)
            dados_processo = await self._extract_dados_processo(page)

            # SAFEGUARD: Garante que tudo √© None se vazio
            try:
                return EventData(
                    reference=reference,
                    tipoEvento=tipo_evento,
                    valores=valores_final,
                    gps=gps,
                    detalhes=detalhes,
                    dataInicio=data_inicio,
                    dataFim=data_fim,
                    imagens=[],  # VAZIO - Stage 3 preenche isto
                    descricao=descricao,
                    observacoes=observacoes,
                    onuselimitacoes=onuselimitacoes,
                    descricaoPredial=descricao_predial,
                    cerimoniaEncerramento=cerimonia,
                    agenteExecucao=agente,
                    dadosProcesso=dados_processo,
                    scraped_at=datetime.utcnow()
                )
            except Exception as e:
                print(f"‚ùå Erro valida√ß√£o EventData para {reference}: {e}")
                raise

        finally:
            await page.close()
            await context.close()

    async def scrape_images_by_ids(
        self,
        references: List[str],
        on_images_scraped: Optional[Callable[[str, List[str]], Awaitable[None]]] = None
    ) -> dict:
        """
        STAGE 3: Scrape apenas imagens para lista de refer√™ncias.

        Args:
            references: Lista de refer√™ncias
            on_images_scraped: Callback async chamado para cada ref com imagens (inser√ß√£o em tempo real)

        Returns:
            Dict: {reference: [image_urls], ...}
        """
        await self.init_browser()

        images_map = {}
        failed = []

        print(f"üñºÔ∏è Stage 3: Scraping imagens de {len(references)} eventos...")

        # Processa em batches paralelos
        for i in range(0, len(references), self.concurrent):
            batch = references[i:i + self.concurrent]

            tasks = [self._scrape_images_only(ref) for ref in batch]
            results = await asyncio.gather(*tasks, return_exceptions=True)

            for idx, result in enumerate(results):
                ref = batch[idx]
                if isinstance(result, list):
                    images_map[ref] = result
                    print(f"  ‚úì {ref}: {len(result)} imagens")

                    # üî• INSER√á√ÉO EM TEMPO REAL via callback
                    if on_images_scraped:
                        try:
                            await on_images_scraped(ref, result)
                        except Exception as e:
                            print(f"  ‚ö†Ô∏è Erro ao atualizar imagens {ref}: {e}")
                else:
                    images_map[ref] = []
                    failed.append(ref)
                    print(f"  ‚úó {ref}: {str(result)[:50]}")

            await asyncio.sleep(self.delay)

        print(f"‚úÖ Stage 3 completo: {len(images_map)} eventos / {len(failed)} falhas")
        return images_map

    async def scrape_volatile_by_ids(
        self,
        references: List[str],
    ) -> List[dict]:
        """
        LIGHTWEIGHT SCRAPER: Only extracts volatile data (price + end time).
        Used by Pipeline X variants for fast price/time updates.

        Args:
            references: Lista de refer√™ncias (ex: ["LO-2024-001", "NP-2024-002"])

        Returns:
            Lista de dicts: [{reference, lanceAtual, dataFim}, ...]
        """
        await self.init_browser()

        results = []
        failed = []

        print(f"‚ö° Volatile scrape: {len(references)} events (price + time only)...")

        # Process in batches
        for i in range(0, len(references), self.concurrent):
            batch = references[i:i + self.concurrent]

            tasks = [self._scrape_volatile_only(ref) for ref in batch]
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)

            for idx, result in enumerate(batch_results):
                if isinstance(result, dict):
                    results.append(result)
                    # Show extracted values
                    price = result.get('lanceAtual')
                    end_time = result.get('dataFim')
                    price_str = f"{price}‚Ç¨" if price is not None else "N/A"
                    time_str = end_time.strftime('%d/%m/%Y %H:%M:%S') if end_time else "N/A"
                    print(f"  ‚úì {result['reference']}: PMA={price_str} | Fim={time_str}")
                else:
                    failed.append(batch[idx])
                    print(f"  ‚úó {batch[idx]}: {str(result)[:50]}")

            await asyncio.sleep(self.delay * 0.5)  # Faster delay for lightweight scrape

        print(f"‚ö° Volatile scrape complete: {len(results)} OK / {len(failed)} failed")
        return results

    async def _scrape_volatile_only(self, reference: str) -> dict:
        """
        Scrape ONLY volatile data from an event page (price + end time).
        Much faster than full scrape - skips GPS, location, descriptions, etc.
        """
        url = f"https://www.e-leiloes.pt/evento/{reference}"

        context = await self.browser.new_context(
            user_agent=self.user_agent,
            viewport={'width': 1920, 'height': 1080}
        )

        page = await context.new_page()

        try:
            # Use domcontentloaded instead of networkidle for faster load
            await page.goto(url, wait_until="domcontentloaded", timeout=10000)
            await asyncio.sleep(0.5)  # Minimal wait

            # Extract only dataFim
            data_fim = None
            try:
                divs = await page.query_selector_all('div.flex.justify-content-between')
                for div in divs:
                    text = await div.text_content()
                    if text and 'Fim:' in text:
                        date_span = await div.query_selector('span.font-semibold')
                        if date_span:
                            value = await date_span.text_content()
                            if value:
                                data_fim = datetime.strptime(value.strip(), '%d/%m/%Y %H:%M:%S')
                        break
            except Exception as e:
                print(f"  ‚ö†Ô∏è Error extracting dataFim for {reference}: {e}")

            # Extract only lanceAtual (P. Mais Alta / Lance Atual) via DOM
            lance_atual = None
            try:
                # Look for the price label spans - try multiple approaches
                price_labels = ['Lance Atual:', 'P. Mais Alta:']

                # Approach 1: Look for label span with specific classes
                spans = await page.query_selector_all('span.text-xl.text-primary-800.font-semibold')
                print(f"  üîç {reference}: Found {len(spans)} spans with text-xl.text-primary-800.font-semibold")

                for span in spans:
                    text = await span.text_content()
                    print(f"    ‚Üí Span text: '{text}'")
                    if text and any(label in text for label in price_labels):
                        # Found the label, now get the sibling with the value
                        parent = await span.evaluate_handle('el => el.parentElement')
                        if parent:
                            value_span = await parent.query_selector('span.text-right')
                            if value_span:
                                value_text = await value_span.text_content()
                                print(f"    ‚Üí Value span text: '{value_text}'")
                                if value_text:
                                    # Parse "83 299,99 ‚Ç¨" or "92 541,54 ‚Ç¨"
                                    value_str = value_text.strip().replace('‚Ç¨', '').replace(' ', '').replace('.', '').replace(',', '.').strip()
                                    lance_atual = float(value_str)
                                    print(f"  üìä {reference}: Found price = {lance_atual}‚Ç¨ (label: {text.strip()})")
                                    break
                            else:
                                print(f"    ‚Üí No span.text-right found in parent")
                        else:
                            print(f"    ‚Üí Could not get parent element")

                # Approach 2: Try alternative - look for any element containing ‚Ç¨ followed by numbers
                if lance_atual is None:
                    print(f"  üîÑ {reference}: Trying alternative approach...")
                    # Get page HTML and search for price pattern
                    html = await page.content()
                    import re
                    # Pattern for Portuguese price format: "123 456,78 ‚Ç¨" or "123 456‚Ç¨"
                    price_match = re.search(r'(?:Lance Atual:|P\. Mais Alta:)\s*</span>.*?>([\d\s.,]+)\s*‚Ç¨', html, re.DOTALL)
                    if price_match:
                        value_str = price_match.group(1).strip().replace(' ', '').replace('.', '').replace(',', '.')
                        lance_atual = float(value_str)
                        print(f"  üìä {reference}: Found price via regex = {lance_atual}‚Ç¨")

                if lance_atual is None:
                    print(f"  ‚ö†Ô∏è {reference}: Could not find price via DOM or regex")
            except Exception as e:
                print(f"  ‚ö†Ô∏è Error extracting lanceAtual for {reference}: {e}")

            return {
                'reference': reference,
                'lanceAtual': lance_atual,
                'dataFim': data_fim
            }

        except Exception as e:
            raise Exception(f"Volatile scrape failed for {reference}: {str(e)}")

        finally:
            await page.close()
            await context.close()

    async def _scrape_images_only(self, reference: str) -> List[str]:
        """Scrape apenas as imagens de um evento usando intercepta√ß√£o de requests"""
        url = f"https://www.e-leiloes.pt/evento/{reference}"

        # Lista para coletar URLs de imagens interceptadas
        intercepted_images = []
        verba_folder = None  # üî• Vamos determinar o folder correto

        context = await self.browser.new_context(
            user_agent=self.user_agent,
            viewport={'width': 1920, 'height': 1080}
        )

        page = await context.new_page()

        # üî• INTERCEPTA requests de imagens da API
        async def handle_route(route):
            nonlocal verba_folder
            request = route.request
            img_url = request.url

            # Intercepta chamadas para Verbas_Fotos/verba_X/
            if 'Verbas_Fotos/verba_' in img_url and img_url.endswith(('.jpg', '.jpeg', '.png', '.webp')):

                # Extrai o folder verba (ex: "verba_121561")
                match = re.search(r'(verba_\d+)', img_url)
                if match:
                    folder = match.group(1)

                    # üéØ Se ainda n√£o determinamos o folder, usa o primeiro
                    if verba_folder is None:
                        verba_folder = folder
                        print(f"    üéØ Folder detectado: {verba_folder}")

                    # ‚úÖ S√ì adiciona imagens do folder CORRETO
                    if folder == verba_folder:
                        if img_url not in intercepted_images:
                            intercepted_images.append(img_url)
                            print(f"    üì∏ {len(intercepted_images)}: {img_url.split('/')[-1]}")
                    else:
                        # ‚ùå Ignora imagens de outros folders
                        print(f"    ‚è≠Ô∏è  Ignorado (folder diferente): {folder}")

            # Continua com o request normal
            await route.continue_()

        # Ativa a intercepta√ß√£o
        await page.route("**/*", handle_route)

        try:
            await page.goto(url, wait_until="networkidle", timeout=15000)

            # ===== AGUARDA dinamicamente baseado no contador de imagens =====
            try:
                footer_selector = '.custom-galleria-footer, .p-galleria-footer, .better-image-badge'
                await page.wait_for_selector(footer_selector, timeout=2000)

                footer = await page.query_selector(footer_selector)
                if footer:
                    footer_text = await footer.inner_text()
                    print(f"    üìä Contador: {footer_text}")

                    # Parse "X/Y" ou "üì∑ Y"
                    match = re.search(r'(\d+)/(\d+)|üì∑\s*(\d+)', footer_text)
                    if match:
                        total_images = int(match.group(2) if match.group(2) else match.group(3))
                        wait_time = (total_images * 2.5) + 1
                        print(f"    ‚è≥ Aguardando {wait_time:.1f}s para {total_images} imagens...")
                        await asyncio.sleep(wait_time)
                    else:
                        await asyncio.sleep(3)
                else:
                    await asyncio.sleep(2)
            except Exception as e:
                print(f"    ‚ö†Ô∏è Erro ao detectar contador: {e}")
                await asyncio.sleep(2)

            # Se interceptamos imagens, retorna essas
            if intercepted_images:
                print(f"    ‚úÖ {len(intercepted_images)} imagens de {verba_folder}")
                return intercepted_images

            # Fallback: usa m√©todo DOM (caso a intercepta√ß√£o falhe)
            print(f"    ‚ö†Ô∏è Nenhuma imagem interceptada, tentando fallback DOM...")
            images = await self._extract_gallery(page)
            return images

        finally:
            await page.close()
            await context.close()
