"""
Web Scraper para E-Leiloes.pt usando Playwright
"""

import sys
import asyncio

# Fix para Windows - asyncio subprocess com Playwright
# CRÃTICO: WindowsProactorEventLoopPolicy suporta subprocessos
if sys.platform == 'win32':
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

from typing import List, Optional
from datetime import datetime
import re
from playwright.async_api import async_playwright, Page, Browser
import os

from models import EventData, GPSCoordinates, EventDetails, ValoresLeilao, ScraperStatus


class EventScraper:
    """Scraper assÃ­ncrono para e-leiloes.pt"""
    
    def __init__(self):
        self.browser: Optional[Browser] = None
        self.playwright = None
        
        # Status tracking
        self.is_running = False
        self.events_processed = 0
        self.events_failed = 0
        self.current_page = None
        self.started_at = None
        self.last_update = None
        
        # Config
        self.delay = float(os.getenv("SCRAPE_DELAY", 0.8))
        self.concurrent = int(os.getenv("CONCURRENT_REQUESTS", 4))
        self.user_agent = os.getenv("USER_AGENT", "Mozilla/5.0 (Windows NT 10.0; Win64; x64)")
    
    async def init_browser(self):
        """Inicializa browser Playwright"""
        if not self.browser:
            self.playwright = await async_playwright().start()
            self.browser = await self.playwright.chromium.launch(
                headless=True,
                args=['--disable-blink-features=AutomationControlled']
            )
    
    async def close(self):
        """Fecha browser"""
        if self.browser:
            await self.browser.close()
        if self.playwright:
            await self.playwright.stop()
    
    async def _scrape_event_details(self, preview: dict, tipo_evento: str) -> EventData:
        """
        FASE 2: Entra na pÃ¡gina individual para extrair detalhes completos
        
        Args:
            preview: Dict com {reference, valores} da listagem
            tipo_evento: "imovel" ou "movel"
        """
        reference = preview['reference']
        valores_listagem = preview['valores']
        
        url = f"https://www.e-leiloes.pt/evento/{reference}"
        
        context = await self.browser.new_context(
            user_agent=self.user_agent,
            viewport={'width': 1920, 'height': 1080}
        )
        
        page = await context.new_page()
        
        try:
            # Navega para pÃ¡gina do evento
            await page.goto(url, wait_until="networkidle", timeout=15000)
            await asyncio.sleep(1.5)

            # Extrai datas do evento
            data_inicio, data_fim = await self._extract_dates(page)

            # GPS (apenas para imÃ³veis)
            gps = None
            if tipo_evento == "imovel":
                gps = await self._extract_gps(page)

            # Detalhes (diferente para imovel vs movel)
            if tipo_evento == "imovel":
                detalhes = await self._extract_imovel_details(page)
            else:
                detalhes = await self._extract_movel_details(page)

            # Confirma/atualiza valores na pÃ¡gina individual (podem ser mais precisos)
            valores_pagina = await self._extract_valores_from_page(page)

            # Merge valores (prioridade: pÃ¡gina individual > listagem)
            valores_final = ValoresLeilao(
                valorBase=valores_pagina.valorBase or valores_listagem.valorBase,
                valorAbertura=valores_pagina.valorAbertura or valores_listagem.valorAbertura,
                valorMinimo=valores_pagina.valorMinimo or valores_listagem.valorMinimo,
                lanceAtual=valores_pagina.lanceAtual or valores_listagem.lanceAtual
            )

            return EventData(
                reference=reference,
                tipoEvento=tipo_evento,
                valores=valores_final,
                gps=gps,
                detalhes=detalhes,
                dataInicio=data_inicio,
                dataFim=data_fim,
                scraped_at=datetime.utcnow()
            )
            
        except Exception as e:
            raise Exception(f"Erro ao scrape {reference}: {str(e)}")
        
        finally:
            await page.close()
            await context.close()
    
    async def _extract_dates(self, page: Page) -> tuple[Optional[datetime], Optional[datetime]]:
        """Extrai datas de inÃ­cio e fim do evento do DOM da pÃ¡gina"""
        try:
            data_inicio = None
            data_fim = None

            # Procura por spans com texto "InÃ­cio:" e "Fim:"
            # As datas aparecem em formato DD/MM/YYYY HH:MM:SS
            spans = await page.query_selector_all('span.text-xs')

            for span in spans:
                text = await span.text_content()
                if not text:
                    continue

                text = text.strip()

                if text == 'InÃ­cio:' or 'InÃ­cio' in text:
                    # Procura pelo prÃ³ximo span com classe font-semibold que contÃ©m a data
                    parent = await span.evaluate_handle('el => el.parentElement')
                    if parent:
                        date_span = await parent.query_selector('span.font-semibold')
                        if date_span:
                            value = await date_span.text_content()
                            if value:
                                try:
                                    # Parse data no formato DD/MM/YYYY HH:MM:SS
                                    data_inicio = datetime.strptime(value.strip(), '%d/%m/%Y %H:%M:%S')
                                except ValueError as e:
                                    print(f"âš ï¸ Erro ao parsear data de inÃ­cio '{value}': {e}")

                elif text == 'Fim:' or 'Fim' in text:
                    parent = await span.evaluate_handle('el => el.parentElement')
                    if parent:
                        date_span = await parent.query_selector('span.font-semibold')
                        if date_span:
                            value = await date_span.text_content()
                            if value:
                                try:
                                    # Parse data no formato DD/MM/YYYY HH:MM:SS
                                    data_fim = datetime.strptime(value.strip(), '%d/%m/%Y %H:%M:%S')
                                except ValueError as e:
                                    print(f"âš ï¸ Erro ao parsear data de fim '{value}': {e}")

            return data_inicio, data_fim

        except Exception as e:
            print(f"âš ï¸ Erro ao extrair datas: {e}")
            return None, None

    async def _extract_gps(self, page: Page) -> GPSCoordinates:
        """Extrai coordenadas GPS do DOM da pÃ¡gina"""
        try:
            latitude = None
            longitude = None

            # Procura por spans com "GPS Latitude:" e "GPS Longitude:"
            spans = await page.query_selector_all('.flex.w-full .font-semibold')

            for span in spans:
                text = await span.text_content()
                if not text:
                    continue

                text = text.strip()

                if text == 'GPS Latitude:':
                    # Pega prÃ³ximo elemento (o valor)
                    next_el = await span.evaluate_handle('el => el.nextElementSibling')
                    if next_el:
                        value = await next_el.text_content()
                        if value:
                            try:
                                latitude = float(value.strip())
                            except ValueError:
                                pass

                elif text == 'GPS Longitude:':
                    next_el = await span.evaluate_handle('el => el.nextElementSibling')
                    if next_el:
                        value = await next_el.text_content()
                        if value:
                            try:
                                longitude = float(value.strip())
                            except ValueError:
                                pass

            return GPSCoordinates(latitude=latitude, longitude=longitude)

        except Exception as e:
            print(f"âš ï¸ Erro ao extrair GPS: {e}")
            return GPSCoordinates(latitude=None, longitude=None)
    
    async def _extract_valores_from_page(self, page: Page) -> ValoresLeilao:
        """Extrai valores da pÃ¡gina individual do evento"""
        valores = ValoresLeilao()
        
        try:
            # Procura por elementos com valores
            body_text = await page.text_content('body')
            
            # Regex para valores monetÃ¡rios
            value_patterns = {
                'valorBase': r'(?:valor\s+base|base)[:\s]*â‚¬?\s*([\d\s.]+,\d{2})',
                'valorAbertura': r'(?:valor\s+abertura|abertura)[:\s]*â‚¬?\s*([\d\s.]+,\d{2})',
                'valorMinimo': r'(?:valor\s+m[iÃ­]nimo|m[iÃ­]nimo)[:\s]*â‚¬?\s*([\d\s.]+,\d{2})',
                'lanceAtual': r'(?:lance\s+atual|atual)[:\s]*â‚¬?\s*([\d\s.]+,\d{2})'
            }
            
            for field, pattern in value_patterns.items():
                match = re.search(pattern, body_text, re.IGNORECASE)
                if match:
                    value_str = match.group(1).replace(' ', '').replace('.', '').replace(',', '.')
                    setattr(valores, field, float(value_str))
                    
        except Exception as e:
            print(f"âš ï¸ Erro ao extrair valores da pÃ¡gina: {e}")
        
        return valores
    
    async def _extract_imovel_details(self, page: Page) -> EventDetails:
        """Extrai detalhes COMPLETOS do IMÃ“VEL via DOM"""
        
        async def extract_detail(label: str) -> str:
            """Extrai valor de um campo especÃ­fico"""
            try:
                # Procura por span.font-semibold com o label
                spans = await page.query_selector_all('.flex.w-full .font-semibold')
                
                for span in spans:
                    text = await span.text_content()
                    if text and text.strip() == label:
                        # Pega prÃ³ximo elemento
                        next_el = await span.evaluate_handle('el => el.nextElementSibling')
                        if next_el:
                            value = await next_el.text_content()
                            return value.strip() if value else "N/A"
                
                return "N/A"
            except:
                return "N/A"
        
        async def extract_area(label: str) -> Optional[float]:
            """Extrai Ã¡rea em mÂ²"""
            try:
                spans = await page.query_selector_all('.flex.w-full .font-semibold')
                
                for span in spans:
                    text = await span.text_content()
                    if text and text.strip() == label:
                        # Procura por span.mr-1 no wrapper
                        wrapper = await span.evaluate_handle('el => el.closest(".flex.w-full")')
                        if wrapper:
                            number_span = await wrapper.query_selector('span.mr-1')
                            if number_span:
                                value = await number_span.text_content()
                                # Remove espaÃ§os e converte para float
                                return float(value.replace(',', '.').strip())
                
                return None
            except:
                return None
        
        # Extrai todos os campos
        tipo = await extract_detail('Tipo:')
        subtipo = await extract_detail('Subtipo:')
        tipologia = await extract_detail('Tipologia:')
        distrito = await extract_detail('Distrito:')
        concelho = await extract_detail('Concelho:')
        freguesia = await extract_detail('Freguesia:')
        
        area_priv = await extract_area('Ãrea Privativa:')
        area_dep = await extract_area('Ãrea Dependente:')
        area_total = await extract_area('Ãrea Total:')
        
        return EventDetails(
            tipo=tipo,
            subtipo=subtipo,
            tipologia=tipologia,
            areaPrivativa=area_priv,
            areaDependente=area_dep,
            areaTotal=area_total,
            distrito=distrito,
            concelho=concelho,
            freguesia=freguesia
        )
    
    async def _extract_movel_details(self, page: Page) -> EventDetails:
        """Extrai detalhes SIMPLIFICADOS do MÃ“VEL (automÃ³vel) via DOM"""
        
        async def extract_detail(label: str) -> str:
            """Extrai valor de um campo especÃ­fico"""
            try:
                spans = await page.query_selector_all('.flex.w-full .font-semibold')
                
                for span in spans:
                    text = await span.text_content()
                    if text and text.strip() == label:
                        next_el = await span.evaluate_handle('el => el.nextElementSibling')
                        if next_el:
                            value = await next_el.text_content()
                            return value.strip() if value else "N/A"
                
                return "N/A"
            except:
                return "N/A"
        
        # Extrai apenas os 3 campos necessÃ¡rios para mÃ³veis
        tipo = await extract_detail('Tipo:')
        subtipo = await extract_detail('Subtipo:')
        matricula = await extract_detail('MatrÃ­cula:')
        
        return EventDetails(
            tipo=tipo,
            subtipo=subtipo,
            matricula=matricula
        )
    
    async def scrape_all_events(self, max_pages: Optional[int] = None) -> List[EventData]:
        """
        Scrape TODOS os eventos (IMOVEIS + MOVEIS) do site
        
        Args:
            max_pages: MÃ¡ximo de pÃ¡ginas para processar POR TIPO (None = todas)
            
        Returns:
            Lista com todos os eventos
        """
        self.is_running = True
        self.started_at = datetime.utcnow()
        self.events_processed = 0
        self.events_failed = 0
        
        all_events = []
        
        await self.init_browser()
        
        try:
            # 1. SCRAPE IMOVEIS (tipo=1)
            print("ğŸ  Iniciando scraping de IMÃ“VEIS...")
            imoveis = await self._scrape_by_type(tipo=1, max_pages=max_pages)
            all_events.extend(imoveis)
            print(f"âœ… ImÃ³veis recolhidos: {len(imoveis)}")
            
            # 2. SCRAPE MOVEIS (tipo=2)
            print("ğŸš— Iniciando scraping de MÃ“VEIS...")
            moveis = await self._scrape_by_type(tipo=2, max_pages=max_pages)
            all_events.extend(moveis)
            print(f"âœ… MÃ³veis recolhidos: {len(moveis)}")
            
            print(f"ğŸ‰ Total de eventos: {len(all_events)}")
            
            return all_events
            
        finally:
            self.is_running = False
    
    async def _scrape_by_type(self, tipo: int, max_pages: Optional[int] = None) -> List[EventData]:
        """
        Scrape eventos de um tipo especÃ­fico (1=imovel, 2=movel)
        
        FASE 1: Extrai referÃªncias + valores da listagem
        FASE 2: Entra em cada evento para detalhes + GPS
        """
        tipo_nome = "imovel" if tipo == 1 else "movel"
        
        # FASE 1: PÃ¡gina de listagem
        events_preview = await self._extract_from_listing(tipo, max_pages)
        print(f"ğŸ“‹ {len(events_preview)} eventos {tipo_nome} encontrados na listagem")
        
        # FASE 2: PÃ¡gina individual (paralelo)
        all_events = []
        
        for i in range(0, len(events_preview), self.concurrent):
            batch = events_preview[i:i + self.concurrent]
            
            tasks = [
                self._scrape_event_details(preview, tipo_nome) 
                for preview in batch
            ]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for result in results:
                if isinstance(result, EventData):
                    all_events.append(result)
                    self.events_processed += 1
                else:
                    self.events_failed += 1
                    print(f"âš ï¸ Erro: {result}")
            
            print(f"ğŸ“Š Processados: {self.events_processed} eventos {tipo_nome}")
            await asyncio.sleep(self.delay)
        
        return all_events
    
    async def _extract_from_listing(self, tipo: int, max_pages: Optional[int]) -> List[dict]:
        """
        FASE 1: Extrai referÃªncias + valores da pÃ¡gina de listagem
        
        Usa paginaÃ§Ã£o com first=0, first=12, first=24, etc. (12 eventos por pÃ¡gina)
        
        Returns:
            Lista de dicts com {reference, valores}
        """
        context = await self.browser.new_context(user_agent=self.user_agent)
        page = await context.new_page()
        
        events_preview = []
        
        try:
            page_num = 0  # ComeÃ§a em 0
            first_offset = 0  # Offset inicial
            
            while True:
                self.current_page = page_num + 1  # Para display (pÃ¡gina 1, 2, 3...)
                
                # Navega para pÃ¡gina de listagem com offset correto
                # https://www.e-leiloes.pt/eventos?layout=grid&first=0&sort=dataFimAsc&tipo=1
                url = f"https://www.e-leiloes.pt/eventos?layout=grid&first={first_offset}&sort=dataFimAsc&tipo={tipo}"
                print(f"ğŸŒ Navegando para pÃ¡gina {page_num + 1} (first={first_offset})...")
                await page.goto(url, wait_until="networkidle")
                await asyncio.sleep(1.5)
                
                # Extrai cards
                cards = await page.query_selector_all('.p-evento')
                
                if not cards:
                    print(f"ğŸ“„ PÃ¡gina {page_num} vazia - fim")
                    break
                
                count_before = len(events_preview)
                
                for card in cards:
                    try:
                        # ReferÃªncia
                        ref_el = await card.query_selector('.pi-tag + span')
                        if not ref_el:
                            continue
                        
                        reference = (await ref_el.text_content()).strip()
                        
                        # Verifica duplicado
                        if any(e['reference'] == reference for e in events_preview):
                            continue
                        
                        # Extrai valores
                        valores = await self._extract_valores_from_card(card)
                        
                        events_preview.append({
                            'reference': reference,
                            'valores': valores
                        })
                        
                    except Exception as e:
                        print(f"âš ï¸ Erro ao extrair card: {e}")
                        continue
                
                count_new = len(events_preview) - count_before
                print(f"ğŸ“„ PÃ¡gina {page_num + 1}: +{count_new} eventos (total: {len(events_preview)})")
                
                if count_new == 0:
                    break
                
                if max_pages and (page_num + 1) >= max_pages:
                    print(f"ğŸ“„ Limite de {max_pages} pÃ¡ginas atingido")
                    break
                
                # Incrementa offset de 12 em 12
                page_num += 1
                first_offset += 12
            
            return events_preview
            
        finally:
            await page.close()
            await context.close()
    
    async def _extract_valores_from_card(self, card) -> ValoresLeilao:
        """Extrai valores (Base, Abertura, MÃ­nimo, Lance Atual) de um card"""
        valores = ValoresLeilao()
        
        try:
            # Procura por spans com classes especÃ­ficas de valores
            value_spans = await card.query_selector_all('span')
            
            for span in value_spans:
                text = await span.text_content()
                if not text:
                    continue
                
                text = text.strip().lower()
                
                # Extrai nÃºmero (formato: "1.234,56 â‚¬" ou "1 234,56 â‚¬")
                match = re.search(r'([\d\s.]+,\d{2})\s*â‚¬', text)
                if not match:
                    continue
                
                value_str = match.group(1).replace(' ', '').replace('.', '').replace(',', '.')
                value = float(value_str)
                
                # Identifica tipo de valor pelo contexto
                parent_text = await card.evaluate('el => el.textContent')
                parent_lower = parent_text.lower()
                
                if 'base' in text or 'base' in parent_lower:
                    valores.valorBase = value
                elif 'abertura' in text or 'abertura' in parent_lower:
                    valores.valorAbertura = value
                elif 'mÃ­nimo' in text or 'minimo' in parent_lower:
                    valores.valorMinimo = value
                elif 'lance' in text or 'atual' in text or 'lance' in parent_lower:
                    valores.lanceAtual = value
                    
        except Exception as e:
            print(f"âš ï¸ Erro ao extrair valores: {e}")
        
        return valores
    
    async def _get_all_references(self, max_pages: Optional[int]) -> List[str]:
        """ObtÃ©m lista de todas as referÃªncias de eventos"""
        context = await self.browser.new_context(user_agent=self.user_agent)
        page = await context.new_page()
        
        references = []
        
        try:
            # Navega para primeira pÃ¡gina
            print("ğŸŒ Navegando para e-leiloes.pt...")
            await page.goto("https://www.e-leiloes.pt/", wait_until="networkidle")
            await asyncio.sleep(2)  # Espera mais tempo para JS carregar
            
            # Extrai total de pÃ¡ginas - MÃšLTIPLOS SELETORES
            total_pages = 1  # Default se nÃ£o encontrar paginador
            
            # Tenta diferentes seletores para o paginador
            paginator_selectors = [
                '.p-paginator-current',
                '.p-paginator .p-paginator-current',
                '[class*="paginator-current"]',
                'span.p-paginator-current',
                '.paginator-text'
            ]
            
            paginator = None
            for selector in paginator_selectors:
                paginator = await page.query_selector(selector)
                if paginator:
                    print(f"âœ… Paginador encontrado com seletor: {selector}")
                    break
            
            if paginator:
                text = await paginator.text_content()
                print(f"ğŸ“‹ Texto do paginador: '{text}'")
                
                # Tenta extrair nÃºmero total de eventos
                match = re.search(r'(\d+)', text.replace(' ', ''))
                if match:
                    # Assume que o Ãºltimo nÃºmero Ã© o total
                    numbers = re.findall(r'\d+', text.replace(' ', ''))
                    total_events = int(numbers[-1]) if numbers else 0
                    total_pages = (total_events + 11) // 12  # 12 por pÃ¡gina
                    
                    if max_pages:
                        total_pages = min(total_pages, max_pages)
                    
                    print(f"ğŸ“„ Total de eventos: {total_events}")
                    print(f"ğŸ“„ Total de pÃ¡ginas calculadas: {total_pages}")
                else:
                    print("âš ï¸ NÃ£o consegui extrair nÃºmeros do paginador")
            else:
                print("âš ï¸ Paginador nÃ£o encontrado com nenhum seletor")
                
                # Conta eventos na pÃ¡gina atual
                cards = await page.query_selector_all('.p-evento')
                print(f"ğŸ“¦ Encontrados {len(cards)} cards na pÃ¡gina atual")
            
            # Se nÃ£o encontrou paginador, tenta descobrir dinamicamente
            if total_pages == 1:
                print("ğŸ” Modo descoberta: tentando encontrar todas as pÃ¡ginas...")
                
                page_num = 1
                while True:
                    self.current_page = page_num
                    
                    # Navega para pÃ¡gina especÃ­fica
                    url = f"https://www.e-leiloes.pt/?page={page_num}"
                    await page.goto(url, wait_until="networkidle")
                    await asyncio.sleep(1.5)
                    
                    # Extrai referÃªncias desta pÃ¡gina
                    cards = await page.query_selector_all('.p-evento')
                    
                    if not cards:
                        print(f"ğŸ“„ PÃ¡gina {page_num} vazia - fim da paginaÃ§Ã£o")
                        break
                    
                    count_before = len(references)
                    
                    for card in cards:
                        ref_element = await card.query_selector('.pi-tag + span')
                        if ref_element:
                            ref = await ref_element.text_content()
                            ref_clean = ref.strip()
                            if ref_clean not in references:  # Evita duplicados
                                references.append(ref_clean)
                    
                    count_new = len(references) - count_before
                    print(f"ğŸ“„ PÃ¡gina {page_num}: +{count_new} eventos (total: {len(references)})")
                    
                    if count_new == 0:
                        print(f"ğŸ“„ Nenhum evento novo na pÃ¡gina {page_num} - fim")
                        break
                    
                    if max_pages and page_num >= max_pages:
                        print(f"ğŸ“„ Atingido limite de {max_pages} pÃ¡ginas")
                        break
                    
                    page_num += 1
                    
            else:
                # Percorre pÃ¡ginas conhecidas
                for page_num in range(1, total_pages + 1):
                    self.current_page = page_num
                    
                    # Navega para pÃ¡gina especÃ­fica
                    url = f"https://www.e-leiloes.pt/?page={page_num}"
                    await page.goto(url, wait_until="networkidle")
                    await asyncio.sleep(1.5)
                    
                    # Extrai referÃªncias desta pÃ¡gina
                    cards = await page.query_selector_all('.p-evento')
                    
                    for card in cards:
                        ref_element = await card.query_selector('.pi-tag + span')
                        if ref_element:
                            ref = await ref_element.text_content()
                            references.append(ref.strip())
                    
                    print(f"ğŸ“„ PÃ¡gina {page_num}/{total_pages}: {len(references)} eventos")
            
            return references
            
        finally:
            await page.close()
            await context.close()
    
    def get_status(self) -> ScraperStatus:
        """Retorna status atual"""
        return ScraperStatus(
            is_running=self.is_running,
            events_processed=self.events_processed,
            events_failed=self.events_failed,
            current_page=self.current_page,
            started_at=self.started_at,
            last_update=self.last_update
        )
